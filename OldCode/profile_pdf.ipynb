{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ec1bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, textwrap\n",
    "import io\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "from xml.sax.saxutils import escape\n",
    "\n",
    "from gpts.gpt_assistants import general_assistant\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.search.documents.models import HybridSearch\n",
    "\n",
    "\n",
    "from openai import AzureOpenAI, APIConnectionError, OpenAI\n",
    "from prompts import new_system_finance_prompt\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "\n",
    "from prompts4 import finance_calculations, finance_pairs, capital_pairs, stakeholders_pairs, biz_overview_pairs, revenue_pairs, default_gpt_prompt, section4a, section4b, section5, section3, biz_overview_web, stakeholders_web\n",
    "from pages.design.func_tools import *\n",
    "from pages.design.formatting import *\n",
    "from pages.design.func_tools import docx_bytes_to_pdf_bytes\n",
    "import re, time\n",
    " \n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# ---- Config (expects the same envs you already used) ----\n",
    "SEARCH_ENDPOINT = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "SEARCH_INDEX    = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "SEARCH_KEY      = os.getenv(\"AZURE_SEARCH_API_KEY\")  # omit if using AAD/RBAC\n",
    "VECTOR_FIELD    = os.getenv(\"VECTOR_FIELD\")\n",
    "TEXT_FIELD      = os.getenv(\"TEXT_FIELD\")\n",
    "\n",
    "AOAI_ENDPOINT   = os.environ[\"AZURE_OPENAI_ENDPOINT\"]            # https://<resource>.openai.azure.com\n",
    "AOAI_API_VER    = os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "AOAI_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]          # e.g., gpt-4o-mini / o3-mini / gpt-5 preview\n",
    "AOAI_KEY        = os.getenv(\"AZURE_OPENAI_API_KEY\")              # omit if using AAD\n",
    "OPENAI_API_KEY  = os.getenv(\"OPENAI_API_KEY\")        # required\n",
    "\n",
    "# ------------------ CODE\n",
    "\n",
    "class profileAgent():\n",
    "\n",
    "    \"\"\"Hybrid (dense+sparse) RAG over Vector Store\n",
    "\n",
    "    This Agent is responsible for creating Company Profiles. \n",
    "    It operates with gpt5.\n",
    "    It is activated by a call on main rag when it is typed 'Create company profile'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, company_name, k, max_text_recall_size, max_chars, model, profile_prompt = new_system_finance_prompt, finance_calculations = finance_calculations):\n",
    "        \n",
    "        self.company_name = company_name\n",
    "\n",
    "        self.k = k\n",
    "        self.max_text_recall_size = max_text_recall_size\n",
    "        self.model = model\n",
    "        self.max_chars = max_chars\n",
    "\n",
    "        self.azure_credentials = AzureKeyCredential(SEARCH_KEY) if SEARCH_KEY else DefaultAzureCredential()\n",
    "        self.search_client = SearchClient(SEARCH_ENDPOINT, SEARCH_INDEX, credential=self.azure_credentials)\n",
    "\n",
    "        self.az_openai = AzureOpenAI(azure_endpoint=AOAI_ENDPOINT, api_key=AOAI_KEY, api_version=AOAI_API_VER)\n",
    "        self.profile_prompt = profile_prompt\n",
    "        self.web_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "        self.reasoning_effort = \"medium\"\n",
    "        self.verbosity = \"medium\"\n",
    "\n",
    "        self.finance_calculations = finance_calculations\n",
    "\n",
    "    def _company_filter(self) -> str:\n",
    "        v = (self.company_name or \"\").replace(\"'\", \"''\").strip()\n",
    "        return f\"company_name eq '{v}'\" if v else None\n",
    "    \n",
    "    def assemble_bm25_from_llm(self, slots: dict) -> str:\n",
    "        def q(s: str) -> str:\n",
    "            # sanitize: remove internal quotes and trim\n",
    "            s = (s or \"\").strip().replace('\"', ' ')\n",
    "            return f\"\\\"{s}\\\"\" if s else \"\"\n",
    "        groups = []\n",
    "\n",
    "        # must-have phrases (ANDed)\n",
    "        for p in slots.get(\"must_have_phrases\", []):\n",
    "            qp = q(p)\n",
    "            if qp:\n",
    "                groups.append(qp)\n",
    "\n",
    "        # metric / statement synonym groups (ORed within each group)\n",
    "        for key in [\"metric\", \"statement\"]:\n",
    "            syns = slots.get(\"synonyms\", {}).get(key, []) or slots.get(key, [])\n",
    "            syns = [q(s) for s in syns if s]\n",
    "            if syns:\n",
    "                groups.append(\"(\" + \" OR \".join(syns) + \")\")\n",
    "\n",
    "        return \" AND \".join(groups) if groups else \"\\\"financial statements\\\"\"\n",
    "\n",
    "\n",
    "    def bm25_creator(self, prompt):\n",
    "\n",
    "        instruction = (\n",
    "            \"Extract finance search slots for Azure AI Search. \"\n",
    "            \"Return strict JSON: {\\\"metric\\\":[], \\\"statement\\\":[], \\\"synonyms\\\":{}, \\\"must_have_phrases\\\":[]} \"\n",
    "            \"(include IFRS/US GAAP variants).\"\n",
    "        )\n",
    "        resp = general_assistant(instruction, prompt, OPENAI_API_KEY, 'gpt-4o')\n",
    "\n",
    "        try:\n",
    "            slots = getattr(resp, \"output_json\", None)\n",
    "            if slots is None:\n",
    "                import json\n",
    "                slots = json.loads(resp.output_text)\n",
    "        except Exception:\n",
    "            # fallback: minimal anchors from prompt\n",
    "            slots = {\"must_have_phrases\": [prompt], \"metric\": [], \"statement\": [], \"synonyms\": {}}\n",
    "        return self.assemble_bm25_from_llm(slots)\n",
    "\n",
    "    def _retrieve_hybrid_enhanced(self, query_nl, k: int = 50, top_n = 30, fields=VECTOR_FIELD, max_text_recall_size:int = 800):\n",
    "        sc = self.search_client\n",
    "        flt = self._company_filter()\n",
    "        \n",
    "        try:\n",
    "            vq = VectorizableTextQuery(text=query_nl, k=k, fields=VECTOR_FIELD)\n",
    "            # Prefer vector-only search (integrated vectorization). If your index isn't set up for it, this raises.\n",
    "            results = sc.search(\n",
    "                search_text=self.bm25_creator(query_nl), \n",
    "                vector_queries=[vq], \n",
    "                top=top_n, \n",
    "                query_type=\"semantic\",\n",
    "                query_caption=\"extractive\", \n",
    "                hybrid_search=HybridSearch(max_text_recall_size=self.max_text_recall_size),\n",
    "                query_caption_highlight_enabled=True,\n",
    "                filter=flt\n",
    "                )\n",
    "            mode = \"hybrid + semantic\"\n",
    "        except HttpResponseError as e:\n",
    "            # Fall back to lexical so you still get results while fixing vector config\n",
    "            results = sc.search(search_text=self.bm25_creator(query_nl), top=k)\n",
    "            mode = f\"lexical (fallback due to: {e.__class__.__name__})\"\n",
    "\n",
    "        hits: List[Dict] = []\n",
    "        for r in results:\n",
    "            d = r.copy() if hasattr(r, \"copy\") else {k2: r[k2] for k2 in r}\n",
    "            d[\"score\"] = d.get(\"@search.reranker_score\") or d.get(\"@search.score\") or 0.0\n",
    "            caps = d.get(\"@search.captions\")\n",
    "            if isinstance(caps, list) and caps:\n",
    "                d[\"caption\"] = getattr(caps[0], \"text\", None)\n",
    "            hits.append(d)\n",
    "\n",
    "        return mode, hits\n",
    "\n",
    "\n",
    "    def _build_context(self, hits: List[Dict], text_field: str = TEXT_FIELD, max_chars: int = 20000):\n",
    "        \"\"\"Build a compact, numbered context block and also return the selected chunk metadata.\"\"\"\n",
    "        lines = []\n",
    "        total = 0\n",
    "        selected = []  # <- we'll return this\n",
    "\n",
    "        for i, h in enumerate(hits, 1):\n",
    "            title     = h.get(\"title\")\n",
    "            chunk_id  = h.get(\"chunk_id\")\n",
    "            full_text = (h.get(text_field) or \"\")\n",
    "            if not full_text:\n",
    "                continue\n",
    "\n",
    "            preview = textwrap.shorten(full_text, width=700, placeholder=\" ...\")\n",
    "            block = f\"[{i}] title={title!r} | chunk_id={chunk_id} | score={h.get('score'):.4f}\\n{full_text}\"\n",
    "\n",
    "            if total + len(block) > self.max_chars:\n",
    "                break\n",
    "\n",
    "            total += len(block)\n",
    "            lines.append(block)\n",
    "\n",
    "            # keep rich metadata so you can show or log it later\n",
    "            selected.append({\n",
    "                \"i\": i,\n",
    "                \"title\": title,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"score\": h.get(\"score\"),\n",
    "                \"caption\": h.get(\"caption\"),\n",
    "                \"preview\": preview,\n",
    "                \"text\": full_text,  # full chunk text (not shortened)\n",
    "                # include any other fields you index, if available:\n",
    "                \"metadata_storage_path\": h.get(\"metadata_storage_path\"),\n",
    "                \"page_number\": h.get(\"page_number\"),\n",
    "                \"doc_type\": h.get(\"doc_type\"),\n",
    "            })\n",
    "\n",
    "        return \"\\n\\n---\\n\\n\".join(lines), selected\n",
    "\n",
    "        \n",
    "    def _generate_pdf(self, text: str) -> bytes:\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        doc = SimpleDocTemplate(buf, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        body = styles[\"BodyText\"]\n",
    "\n",
    "        story = []\n",
    "        # Treat double newlines as paragraph breaks; keep single newlines as <br/>\n",
    "        for para in (text or \"\").split(\"\\n\\n\"):\n",
    "            safe = escape(para).replace(\"\\n\", \"<br/>\")\n",
    "            story.append(Paragraph(safe if safe.strip() else \"&nbsp;\", body))\n",
    "            story.append(Spacer(1, 8))\n",
    "\n",
    "        doc.build(story)\n",
    "        buf.seek(0)\n",
    "        return buf.getvalue()\n",
    "    \n",
    "    def _extract_cited_idxs(self, answer: str) -> list[int]:\n",
    "        # Matches [#1], [#12], etc. (also tolerates stray [1])\n",
    "        nums = set(int(n) for n in re.findall(r\"\\[#?(\\d+)\\]\", answer))\n",
    "        return sorted(nums)\n",
    "\n",
    "    def _rag_answer(self, rag_nl, question, k: int = 5, temperature: float = 0.2):\n",
    "\n",
    "        # question = f'CREATE A SECTION OF COMPANY PROFILE USING LAST YEARS OF ANNUAL REPORT PRESENT IN THE CONTEXT FOR {self.company_name}. IF ANY INFORMATION IS NOT FOUND STATE AS n.a. .\\n\\n THIS IS THE SECTION TO BE BUILT: \\n {section7}  \\n USE THIS TO GUIDE YOURSELF ON SEMANTIC TERMS AND HOW TO CALCULATE: \\n {finance_calculations}'\n",
    "        \n",
    "        mode, hits = self._retrieve_hybrid_enhanced(\n",
    "            # query=rag_q, \n",
    "            query_nl=rag_nl,\n",
    "            k=25\n",
    "            )\n",
    "        ctx_text, ctx_items = self._build_context(hits)\n",
    "\n",
    "        system_msg = self.profile_prompt + (\n",
    "            \"\\nWhen you use a fact from the context, add citations like [#1], [#2].\"\n",
    "            \"\\nOnly rely on the numbered context; if a value is missing, say 'n.a.'.\"\n",
    "            f\"\\nIF ANY INFORMATION IS NOT FOUND STATE AS n.a. .\\n\\n USE THIS TO GUIDE YOURSELF ON SEMANTIC TERMS AND HOW TO CALCULATE: \\n {finance_calculations}\"\n",
    "        )\n",
    "        user_msg = f\"Question:\\n{question}\\n\\nContext snippets (numbered):\\n{ctx_text}\"\n",
    "\n",
    "        client = self.az_openai\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\",   \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "        # Try streaming first (SSE). Some networks/proxies block streaming; if so, fall back.\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=AOAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            reasoning_effort=\"high\"\n",
    "        )\n",
    "        answer = resp.choices[0].message.content\n",
    "        mode_model = \"non-streaming (fallback)\"\n",
    "\n",
    "        cited = self._extract_cited_idxs(answer)\n",
    "        used_chunks = [c for c in ctx_items if c[\"i\"] in cited]\n",
    "\n",
    "        # return self._generate_pdf(answer)\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"citations\": cited,          # [1, 3, 7]\n",
    "            \"used_chunks\": used_chunks,  # detailed dicts for each cited snippet\n",
    "            \"all_chunks\": ctx_items,     # everything you sent (optional)\n",
    "            \"mode\": mode                 # retrieval mode info (optional)\n",
    "        }\n",
    "\n",
    "    def _web_search(self, messages):\n",
    "        resp = self.web_openai.responses.create(\n",
    "            model='gpt-5',\n",
    "            input=messages,\n",
    "            tools=[{\"type\": \"web_search\"}],\n",
    "            tool_choice=\"auto\",\n",
    "            # max_output_tokens=self.max_output_tokens,\n",
    "            reasoning={\"effort\": self.reasoning_effort},\n",
    "            text={\"verbosity\": self.verbosity},\n",
    "        )\n",
    "        \n",
    "        return resp.output_text\n",
    "    \n",
    "    def _answer(self, question, ctx_text, k: int = 5, temperature: float = 0.2):\n",
    "\n",
    "        system_msg = self.profile_prompt + (\n",
    "            \"\\nWhen you use a fact from the context, preserve any existing citations like [#1], [#2], [#5, p.41] that are already in the context text.\"\n",
    "            \"\\nOnly rely on the provided context; if a value is missing, say 'n.a.'.\"\n",
    "            \"\\nIMPORTANT: If the formatting instructions request a Sources section, you MUST include it at the end.\"\n",
    "            \"\\nFor the Sources section, list all citation numbers/references that appear in your answer, and describe what document/source each refers to based on information in the context.\"\n",
    "        )\n",
    "        user_msg = f\"Question:\\n{question}\\n\\nContext snippets:\\n{ctx_text}\"\n",
    "\n",
    "        client = self.az_openai\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\",   \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "        # Try streaming first (SSE). Some networks/proxies block streaming; if so, fall back.\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=AOAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            reasoning_effort=\"high\"\n",
    "        )\n",
    "        answer = resp.choices[0].message.content\n",
    "\n",
    "        cited = self._extract_cited_idxs(answer)\n",
    "\n",
    "        # return self._generate_pdf(answer)\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"citations\": cited,          # [1, 3, 7]\n",
    "        }   \n",
    "    \n",
    "    @staticmethod\n",
    "    def has_na(text: str) -> bool:\n",
    "        # match \"n.a.\" or \"n/a\" (case-insensitive)\n",
    "        return bool(re.search(r\"\\b(n\\.a\\.|n/a)\\b\", text, flags=re.I))\n",
    "\n",
    "    def _sections(self, pairs):\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        max_extra_na_retries = 1        # try again at most 2 times (total <= 3 calls per item)\n",
    "        base_delay_seconds = 3.0        # polite delay between attempts\n",
    "\n",
    "\n",
    "        for q, r in pairs:\n",
    "            tries = 0\n",
    "            while True:\n",
    "                if tries > 0:\n",
    "                    # small incremental delay before re-trying\n",
    "                    time.sleep(base_delay_seconds + 0.5 * tries)\n",
    "\n",
    "                resp = self._rag_answer(rag_nl=r[0], question=q[0])\n",
    "                answer_text = resp[\"answer\"]\n",
    "\n",
    "                # stop if good answer OR we've exhausted retries\n",
    "                if not profileAgent.has_na(answer_text) or tries >= max_extra_na_retries:\n",
    "                    answers.append(answer_text)\n",
    "                    break\n",
    "\n",
    "                # otherwise, try again\n",
    "                tries += 1\n",
    "\n",
    "            # optional small gap between different (r,q) items\n",
    "            time.sleep(5.0)\n",
    "        \n",
    "        return answers\n",
    "    \n",
    "    def _generate_section(self, section):\n",
    "\n",
    "        if section == 'GENERATE BUSINESS OVERVIEW':\n",
    "            # =========== GENERATE BUSINESS OVERVIEW\n",
    "            biz_overview_pairs_flat = list(zip(biz_overview_pairs[1], biz_overview_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs = biz_overview_pairs_flat)\n",
    "\n",
    "            #getting web search sections\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{biz_overview_web} \\n\\n Mention in the Beggining of the answer that this is WEBSEARCH SOURCE'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp_web = self._web_search(messages)\n",
    "\n",
    "            section_built.append(resp_web)\n",
    "\n",
    "            # Join all context sections - they already contain their own citations\n",
    "            # Just concatenate them so the model can synthesize\n",
    "            ctx_text_formatted = \"\\n\\n\".join(section_built)\n",
    "\n",
    "            resp = self._answer(question=biz_overview_mix_formatting, ctx_text=ctx_text_formatted)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE KEY STAKEHOLDERS':\n",
    "        # =========== GENERATE KEY STAKEHOLDERS\n",
    "            stakeholders_pairs_flat = list(zip(stakeholders_pairs[1], stakeholders_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs= stakeholders_pairs_flat)\n",
    "\n",
    "            #getting web search sections\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{stakeholders_web} \\n\\n Mention in the Beggining of the answer that this is WEBSEARCH SOURCE'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp_web = self._web_search(messages)\n",
    "\n",
    "            section_built.append(resp_web)\n",
    "\n",
    "            # Join all context sections - they already contain their own citations\n",
    "            # Just concatenate them so the model can synthesize\n",
    "            ctx_text_formatted = \"\\n\\n\".join(section_built)\n",
    "\n",
    "            resp = self._answer(question=stakeholders_web_mix, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE FINANCIAL HIGHLIGHTS':\n",
    "            # =========== GENERATE FINANCIAL HIGHLIGHTS\n",
    "            finance_pairs_flat = list(zip(finance_pairs[1], finance_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs=finance_pairs_flat)\n",
    "            resp = self._answer(question=finance_formatting_2, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE CAPITAL STRUCTURE':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            capital_pairs_flat = list(zip(capital_pairs[1], capital_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs= capital_pairs_flat)\n",
    "            resp = self._answer(question=capital_structure_formatting_2, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE REVENUE SPLIT':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            revenue_pairs_flat = list(zip(revenue_pairs[1], revenue_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs= revenue_pairs_flat)\n",
    "            resp = self._answer(question=revenue_split_formatting, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE PRODUCTS SERVICES OVERVIEW':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{products_overview_formatting}'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp = self._web_search(messages)\n",
    "            return resp \n",
    "        elif section == 'GENERATE GEO FOOTPRINT':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{geo_footprint_formatting}'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp = self._web_search(messages)\n",
    "            return resp\n",
    "        elif section == 'GENERATE DEVELOPMENTS HIGHLIGHTS':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{key_devs_formatting}'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp = self._web_search(messages)\n",
    "            return resp\n",
    "\n",
    "\n",
    "    def generate_company_profile(self):\n",
    "\n",
    "        # =========== GENERATE BUSINESS OVERVIEW\n",
    "        resp = self._generate_section('GENERATE BUSINESS OVERVIEW')\n",
    "        doc = insert_biz_overview(resp['answer'])\n",
    "\n",
    "        time.sleep(60)\n",
    "        # =========== GENERATE KEY STAKEHOLDERS\n",
    "        resp = self._generate_section('GENERATE KEY STAKEHOLDERS')\n",
    "        doc = insert_stakeholders(resp['answer'], doc=doc)\n",
    "        \n",
    "        time.sleep(60)\n",
    "        # =========== GENERATE FINANCIAL HIGHLIGHTS\n",
    "        resp = self._generate_section('GENERATE FINANCIAL HIGHLIGHTS')\n",
    "        doc = insert_finance(resp['answer'], doc=doc)\n",
    "\n",
    "        time.sleep(60)\n",
    "        # =========== GENERATE CAPITAL STRUCTURE\n",
    "        resp = self._generate_section('GENERATE CAPITAL STRUCTURE')\n",
    "        doc = insert_capital_structure(resp['answer'], doc=doc)\n",
    "\n",
    "        pdf_bytes = docx_bytes_to_pdf_bytes(doc)\n",
    "\n",
    "        return pdf_bytes\n",
    "        # =========== UNION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73093a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts4 import section7, finance_calculations, system_mod\n",
    "import time\n",
    "import re, time\n",
    "\n",
    "company = 'SEAPORT_TOPCO_LIMITED'\n",
    "sys = system_mod\n",
    "calc = finance_calculations\n",
    "\n",
    "agent = profileAgent(\n",
    "    company_name = company,\n",
    "    k=50, \n",
    "    max_text_recall_size=35, \n",
    "    max_chars=10000,\n",
    "    model='gpt-5', \n",
    "    profile_prompt= sys,\n",
    "    finance_calculations= calc\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c207a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "1. Business Overview\n",
      "\n",
      "- Seaport Topco Limited is the UK holding company for a life sciences tools group trading as SPT Labtech that designs and manufactures automated instruments, consumables and related support services to help laboratories increase productivity and reproducibility in research [#1].\n",
      "- The company focuses on automation and miniaturisation across high‑impact segments of life science research and has a long operating history through SPT Labtech, which was founded in 1997, spun out from TTP Group in 2018 and acquired by EQT Private Equity in 2022 to support global expansion and innovation [#1].\n",
      "- It offers a portfolio spanning liquid handling, sample preparation for cryo‑EM and structural biology and sample management, with recognised product families including mosquito, dragonfly, firefly, apricot, BioMicroLab, chameleon and Quantifoil [#2].\n",
      "- The company operates with global R&D and manufacturing in Melbourn UK and has additional production, commercial hubs and offices in Covina California USA, Jena Germany, Tokyo Japan, Hangzhou and Shanghai China and Delhi India [#2].\n",
      "- Its customers are scientists and researchers across global biopharma, biotechnology, academic and government laboratories and contract research organizations [#2].\n",
      "- The company faces stress triggers from an FY24 operating loss of £20.7m on £76.8m revenue and from floating‑rate EUR and USD term loans with bullet maturities in Aug-29 that heighten refinancing and interest exposure, while new shareholder loans introduced in Mar-25 add leverage ahead of a Mar-30 maturity [#1].\n",
      "\n",
      "Sources:\n",
      "- [#1] Seaport Topco Limited – Companies House filing (FY24 accounts), Strategic Report and Notes to borrowings; substantiates group description, history, FY24 revenue and operating loss, and capital structure including EUR and USD term loans due Aug-29, floating rates, £30.0m RCF and shareholder loans added in Mar-25 maturing Mar-30; https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf\n",
      "- [#2] SPT Labtech – Company page; substantiates product portfolio, global operations footprint and customer base; https://www.sptlabtech.com/company\n"
     ]
    }
   ],
   "source": [
    "biz_ov = agent._generate_section('GENERATE BUSINESS OVERVIEW')\n",
    "\n",
    "print(biz_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f10593fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Key Stakeholders\n",
      "\n",
      "| Metric | Details |\n",
      "| --- | --- |\n",
      "| Shareholders | - Immediate parent company: EQT Jupiter Luxco S.à r.l. (Luxembourg) [1] [4]  <br> - Ultimate parent company: n/a (not disclosed; notes state the smallest and largest group in which results are consolidated is headed by Seaport Topco Limited) [1] [4] |\n",
      "| Management | - Chairman: Kieran Murphy (Non‑Executive Chair; per SPT Labtech website)  <br> - Chief Executive Officer: Rob Walton (appointed CEO May-24 per website; appointed director of Seaport Topco Limited on 22-Jul-24 and signed FY24 Directors’ Report on 29-Apr-25) [#3 p.12] [#6 p.16]  <br> - Chief Financial Officer: Andrew Holford (Group CFO; joined Dec-24 per website)  <br> - Directors per FY24 Directors’ Report: M Bauer; R Diggelmann — resigned 25-Dec-24; P Dowdy — resigned 24-Feb-25; J Feldman; R Friel — resigned 3-Mar-25; K Murphy; D Newble — resigned 8-Jul-24; A Thorburn; R Walton — appointed 22-Jul-24 [#3 p.12] |\n",
      "| Lenders | - External lender names: n.a. (FY24/FY23 statements disclose “bank loans” but do not identify lending banks) [2] [5]  <br> - Note: A £75.0m delayed draw term loan (DDTL) was unutilised and access was removed in Mar-25; lenders not named [5] |\n",
      "| Auditors | n.a. (auditor name not available in the provided excerpts of the Independent Auditors’ Report) |\n",
      "| Advisors | - Facility Agent (under the senior facilities agreement dated 05-Aug-22): Kroll Agency Services Limited [#1] [#2]  <br> - 2022 M&A advisors on EQT’s acquisition of SPT Labtech (contextual group transaction):  <br>   • Buy-side (EQT): Evercore (M&A), Kirkland & Ellis (Legal), Deloitte (Financial & Tax), BCG (Commercial), Ringstone (Technology), The Footprint Firm (ESG)  <br>   • Sell-side (Battery/Company): J.P. Morgan (exclusive financial advisor), Charles Russell Speechlys and SecondSight Law (Legal) |\n",
      "| Charges | No outstanding charges recorded at Companies House for Seaport Topco Limited; n/a for issue date and persons entitled (no charges listed) |\n",
      "\n",
      "Sources\n",
      "\n",
      "- [1] Seaport Topco Limited FY23 Annual Report – Notes to the Financial Statements, “Controlling party,” p. 52. Link (filed 25-Sep-24): https://aiprojecetteneo.blob.core.windows.net/companieshouseinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_1.pdf\n",
      "- [4] Seaport Topco Limited FY22 Annual Report – Notes to the Financial Statements, “Controlling party,” p. 46. Link (filed 15-Oct-23): https://aiprojecetteneo.blob.core.windows.net/companieshouseinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2023-10-15_2.pdf\n",
      "- [#3 p.12] Seaport Topco Limited Annual Report (year ended 31-Dec-24) – Directors’ Report, p. 12 (Directors and changes). Link (filed 30-Sep-25): https://aiprojectteneo.blob.core.windows.net/companieshousesinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n",
      "- [#6 p.16] Seaport Topco Limited Annual Report (year ended 31-Dec-24) – Directors’ Report approval/signature by R H Walton, p. 16. Link (filed 30-Sep-25): https://aiprojectteneo.blob.core.windows.net/companieshousesinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n",
      "- [2] Seaport Topco Limited Annual Report (filed Sep-25) – Notes 20 & 21 (Creditors: amounts falling due within one year / after more than one year) – bank loans disclosed; lender names not provided – p. 41. Link: https://aiprojectteneo.blob.core.windows.net/companieshouse-singlefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n",
      "- [5] Seaport Topco Limited Annual Report (filed Sep-25) – Note 30 (Analysis of net debt) and Note 32 (Subsequent events: removal of £75m DDTL; related‑party loans). Link: https://aiprojectteneo.blob.core.windows.net/companieshouse-singlefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n",
      "- [#1] Seaport Topco Limited Annual Report and Financial Statements, Year ended 31-Dec-2024 (published Sep-25) — Note 22: Loans (continued), “with Kroll Agency Services Limited acting as the facility agent” (PDF p. 67). Link: https://aiprojectteneo.blob.core.windows.net/companieshouselinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n",
      "- [#2] Seaport Topco Limited Annual Report and Financial Statements, Year ended 31-Dec-2023 (published Sep-24) — Note 23: Loans, “with Kroll Agency Services Limited acting as the facility agent” (PDF p. 72). Link: https://aiprojectteneo.blob.core.windows.net/companieshouselinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_1.pdf\n",
      "- SPT Labtech – CEO appointment news (Rob Walton). Link: https://www.sptlabtech.com/news/rob-walton-appointed-as-new-chief-executive-officer\n",
      "- SPT Labtech – Executive Leadership (CFO: Andrew Holford). Link: https://www.sptlabtech.com/executive-leadership\n",
      "- EQT Group press release (Jun-22): “EQT Private Equity to acquire SPT Labtech…” (transaction advisors listed). Link: https://eqtgroup.com/news/eqt-private-equity-to-acquire-spt-labtech-a-fast-growing-laboratory-automation-player-focused-on-low-volume-liquid-handling-technology-for-gbp-650-million-from-battery-ventures-2022-06-22\n",
      "- Companies House – Seaport Topco Limited (Company no. 14171962) — Charges page (no outstanding charges). Link: https://find-and-update.company-information.service.gov.uk/company/14171962\n"
     ]
    }
   ],
   "source": [
    "key_stake = agent._generate_section('GENERATE KEY STAKEHOLDERS')\n",
    "\n",
    "print(key_stake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6389204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "6. Financial Highlights\n",
      "\n",
      "| Metric | FY24 | FY23 | FY22 |\n",
      "| --- | --- | --- | --- |\n",
      "| Revenue (Turnover) | £76.8m [1] | £81.4m [2] | £32.8m [2] |\n",
      "| Revenue growth % (yoy) | -5.7% [1][2] | +148.1% [2] | n.a. |\n",
      "| Gross profit | £43.9m [1] | £48.4m [2] | £14.3m [2] |\n",
      "| Gross margin % | 57.2% [1][2] | 59.5% [1][2] | 43.7% [2] |\n",
      "| EBITDA | £10.7m [1] | £20.2m [2] | -£1.2m [2] |\n",
      "| EBITDA margin % | 14.0% [1][2] | 24.8% [1][2] | n.m. |\n",
      "| Net working capital (cash flow movement) | -£1.2m [1] | £3.9m [2] | -£3.0m [2] |\n",
      "| Cash flow from operating activities excl. NWC | £10.8m [1] | £11.4m [2] | £1.2m [2] |\n",
      "| Capex | -£2.9m [1] | -£7.2m [2] | -£0.9m [2] |\n",
      "| Other cash flow from investing activities | £0.2m [1] | -£0.8m [2] | -£638.1m [2] |\n",
      "| CFADS | £6.9m [1][2] | £7.3m [2] | -£640.7m [2] |\n",
      "| Net cash from financing activities | -£6.7m [1] | -£4.2m [1] | £657.6m [2] |\n",
      "| Bank loan drawdowns (cash) | £17.9m [1] | £12.1m [1] | £187.3m [2] |\n",
      "| Other loans drawdowns (cash) | £7.7m [1] | £0.0m [1] | n.a. |\n",
      "| Repayment of bank loans (cash) | -£8.6m [1] | £0.0m [1] | n.a. |\n",
      "| Equity issued (cash) | £1.5m [1] | £0.4m [1] | £484.2m [2] |\n",
      "| Interest paid (cash) | -£25.2m [1] | -£16.6m [1] | -£4.9m [2] |\n",
      "| Opening cash | £11.8m [1] | £9.0m [1] | n.a. |\n",
      "| Change in cash | £0.2m [1] | £3.2m [2] | £16.9m [2] |\n",
      "| Closing cash | £11.8m [1] | £11.8m [2] | £9.0m [2] |\n",
      "| Gross external debt (bank + other loans) | £198.4m [1] | £183.4m [2] | £174.4m [2] |\n",
      "| Total Debt (bank debt + lease liabilities) | n.a. | n.a. | n.a. |\n",
      "| Net Debt | £186.6m [1][2] | £171.6m [2] | £165.5m [2] |\n",
      "| Leverage (Net Debt / EBITDA) | 17.4x [1][2] | 8.5x [2] | n.m. |\n",
      "\n",
      "Summary / Interpretatio\n",
      "- Revenue decreased by 5.7% to £76.8m in FY24 after surging 148.1% to £81.4m in FY23. Management attributes the FY24 decline primarily to a smaller instruments order book entering the year, partially offset by recurring revenue growth [1]. \n",
      "- Gross profit reduced to £43.9m in FY24 from £48.4m in FY23, with gross margin narrowing to 57.2% from 59.5%. The margin compression mirrors the lower sales in FY24, though margins remain structurally higher than FY22’s 43.7% [1][2].\n",
      "- EBITDA fell to £10.7m in FY24 from £20.2m in FY23 and was negative in FY22, with EBITDA margin declining to 14.0% from 24.8%. Management links the FY24 EBITDA decline to the lower revenue base, with recurring revenue growth only partly offsetting the drop in instruments activity [1].\n",
      "- Net working capital was a £1.2m cash outflow in FY24 versus a £3.9m inflow in FY23 and a £3.0m outflow in FY22. The FY24 outflow was driven by a £4.4m increase in debtors, partly offset by a £2.2m decrease in stocks and a £1.1m increase in creditors [1][2].\n",
      "- Capex outflows moderated to £2.9m in FY24 from £7.2m in FY23, compared with £0.9m in FY22. The step‑down in FY24 indicates lower investment intensity year over year [1][2].\n",
      "- CFADS was broadly stable year over year at £6.9m in FY24 versus £7.3m in FY23, a significant recovery from the -£640.7m in FY22 that coincided with very large other investing outflows in that year [1][2].\n",
      "- Financing cash flows swung from a very large net inflow in FY22 (£657.6m) to net outflows in FY23 (£4.2m) and FY24 (£6.7m). In FY24, £17.9m of bank loan drawdowns and £7.7m of other loans were more than offset by £8.6m of bank loan repayments and £25.2m of cash interest, with £1.5m of equity issued providing a modest offset [1][2].\n",
      "- Gross external debt (bank and other loans) increased to £198.4m in FY24 from £183.4m in FY23, driving net debt up to £186.6m from £171.6m. Total Debt inclusive of lease liabilities is not available in the disclosures used. Leverage rose sharply to 17.4x in FY24 from 8.5x in FY23 due to the fall in EBITDA; FY22 leverage is not meaningful given negative EBITDA [1][2].\n",
      "\n",
      "Sources\n",
      "- [1] Seaport Topco Limited Annual Report (file dated 2025-09-30; year ended 31-Dec-24). Pages used: p. 6 (EBITDA/Adjusted EBITDA commentary), p. 12 (Consolidated Statement of Comprehensive Income: revenue, cost of sales, gross profit), p. 13 (cash and cash equivalents), p. 17–18 (Consolidated Statement of Cash Flows: net cash from operating activities, working-capital movements, investing and financing cash flows; opening/closing cash; net change), p. 76 (Note 30 Analysis of net debt: bank loans and other loans). Link: https://aiprojectteneo.blob.core.windows.net/companieshousesinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n",
      "- [2] Seaport Topco Limited Annual Report (file dated 2024-09-25; year ended 31-Dec-23, incl. restated FY22 comparatives). Pages used: p. 9 (KPI table including EBITDA for FY23 and restated FY22), p. 13 (Consolidated Statement of Comprehensive Income: revenue, cost of sales, gross profit for FY23 and FY22), p. 18–19 (Consolidated Statement of Cash Flows: operating, investing and financing cash flows; opening/closing cash; net change), p. 52 (Note 31 Analysis of net debt: bank loans and cash for FY23 and FY22). Link: https://aiprojectteneo.blob.core.windows.net/companieshousesinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_1.pdf\n"
     ]
    }
   ],
   "source": [
    "fin_high = agent._generate_section('GENERATE FINANCIAL HIGHLIGHTS')\n",
    "print(fin_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a1983ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. Capital Structure\n",
      "\n",
      "| Metric | FY24 |\n",
      "| --- | --- |\n",
      "| Facility Name | Senior Facilities Agreement — Facility B1 (EUR) and Facility B2 (USD); Revolving Credit Facility |\n",
      "| Interest Rate | B1: EURIBOR + 6.25%; B2: Term SOFR + 6.25%; RCF: n.a. [#1] |\n",
      "| Maturity | B1/B2: Aug-29; RCF: n.a. [#1] |\n",
      "| Adjusted EBITDA | £12.9m [#3] |\n",
      "| Cash (Closing Cash) | £11.8m [#4] |\n",
      "| Net Debt | £186.7m [#5] |\n",
      "| Liquidity | £16.6m [#1][#2][#4] |\n",
      "| Leverage (Net Debt/EBITDA) | 14.5x [#5][#3] |\n",
      "| Facility B1 outstanding (GBP) | £34.2m [#1] |\n",
      "| Facility B2 outstanding (GBP) | £137.2m [#1] |\n",
      "| RCF drawn | £25.2m [#2] |\n",
      "| RCF facility size | £30.0m [#1] |\n",
      "| Delayed Drawdown Facility size | n.a. |\n",
      "| Bank loans due after >5 years | n.a. |\n",
      "| Bank loans due within 1 year | n.a. |\n",
      "| Bank loans + RCF outstanding (excl. leases) | £196.6m [#1][#2] |\n",
      "\n",
      "Summary / Interpretation\n",
      "- Net debt is £186.7m against Adjusted EBITDA of £12.9m, resulting in very high leverage of 14.5x, indicating that debt is large relative to earnings and that deleveraging capacity from operations is limited; the £196.6m of bank loans and RCF outstanding further underscores the debt load relative to cash generation [#5][#3][#1][#2].\n",
      "- The table does not indicate any new refinancing or additional committed facilities in FY24; the absence of a delayed draw facility and the continued reliance on B1/B2 term loans and the RCF suggest no material refinancing actions were completed in the period.\n",
      "- Debt covenant terms and the results of any covenant tests are not disclosed in the table, so covenant headroom and potential pressure cannot be assessed from the provided data.\n",
      "- The collateral/security package is not set out in the table, therefore no conclusions can be drawn here about the security structure or creditor ranking from the provided information.\n",
      "- Liquidity is modest at £16.6m, comprised of £11.8m cash and approximately £4.8m of undrawn RCF (based on a £30.0m facility with £25.2m drawn); no overdraft or accordion capacity is indicated, which limits flexibility if cash flows tighten [#1][#2][#4].\n",
      "- The next known maturities are the bullet repayments of the term loans in Aug-29; no bank loans due within one year are shown and the RCF maturity is not specified in the data provided, indicating limited near-term amortizations but a concentrated 2029 refinancing requirement [#1].\n",
      "- Debt-related risk is elevated: leverage of 14.5x is high and liquidity is thin relative to total borrowings; with floating-rate margins of +6.25% on both term loans, interest costs may remain significant, increasing refinancing and cash flow risk if earnings soften before the 2029 maturities [#1][#5][#3][#1][#2][#4].\n",
      "\n",
      "Sources\n",
      "- [#1] Seaport Topco Limited Annual Report FY24 (year ended 31-Dec-24), Note 22 “Loans”, p. 67 — Supports facility terms, B1/B2 maturities (Aug-29), B1/B2 outstanding amounts and RCF facility size.\n",
      "- [#2] Seaport Topco Limited Annual Report FY24, Note 22 “Loans”, p. 68 — Supports RCF drawn at year-end (£25.2m) and multi-currency breakdown.\n",
      "- [#3] Seaport Topco Limited Annual Report FY24, Group Strategic Report, p. 6 — Supports FY24 Adjusted EBITDA (£12.9m).\n",
      "- [#4] Seaport Topco Limited Annual Report FY24, Financial Statements/Group Strategic Report (p. 8 or SOFP p. 13) — Supports closing cash at 31-Dec-24 (£11.8m).\n",
      "- [#5] Seaport Topco Limited Annual Report 2025 (covering FY24), Note 30 “Analysis of net debt”, p. 76 — Supports net debt at 31-Dec-24 (£186.7m).\n"
     ]
    }
   ],
   "source": [
    "cap_stru = agent._generate_section('GENERATE CAPITAL STRUCTURE')\n",
    "\n",
    "print(cap_stru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c0677a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Revenue Split\n",
      "\n",
      "Revenue by geography (country of destination) [#5 p.53]\n",
      "| Segment | Revenue (£m) | Share (%) |\n",
      "| --- | --- | --- |\n",
      "| UK | 9.2 | 12.0 |\n",
      "| Europe | 15.9 | 20.7 |\n",
      "| North America | 35.1 | 45.7 |\n",
      "| Rest of world | 16.7 | 21.7 |\n",
      "| Total | 76.8 | 100.0 |\n",
      "\n",
      "Revenue by activity (class of business) [#5 p.53]\n",
      "| Segment | Revenue (£m) | Share (%) |\n",
      "| --- | --- | --- |\n",
      "| Sale of goods | 60.5 | 78.8 |\n",
      "| Services | 16.3 | 21.2 |\n",
      "| Total | 76.8 | 100.0 |\n",
      "\n",
      "\n",
      "Sources\n",
      "- [#5] Seaport Topco Limited FY24 Annual Report, Note 5 Turnover & Other operating income (p.53): revenue by destination and by class of business for FY24. Link: https://aiprojectteneo.blob.core.windows.net/companieshouselinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf\n"
     ]
    }
   ],
   "source": [
    "rev_split = agent._generate_section('GENERATE REVENUE SPLIT')\n",
    "\n",
    "print(rev_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feb08765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4a. Products/Services Overview\n",
      "\n",
      "1 - SUMMARY\n",
      "Based on the latest filed Group accounts for FY24 (approved Apr-25) and the official product sites of the Group’s operating subsidiaries, Seaport Topco Limited provides instruments, consumables and automation solutions focused on automated liquid handling, sample preparation (including cryo‑EM) and sample management for life science research.\n",
      "\n",
      "- firefly (incl. firefly+): an all‑in‑one compact liquid handling platform that combines pipetting, dispensing, incubation and shaking to automate NGS library and sample preparation workflows. ([sptlabtech.com](https://www.sptlabtech.com/products/firefly?utm_source=openai))\n",
      "- mosquito family (Gen3, LV genomics, HV genomics, Xtal3, LCP): positive‑displacement nanoliter pipetting instruments for genomics miniaturization and protein crystallography, delivering accurate 25 nL–5 µL transfers with disposable micropipettes. ([sptlabtech.com](https://www.sptlabtech.com/products/mosquito?utm_source=openai))\n",
      "- dragonfly discovery: multi‑channel, non‑contact positive‑displacement dispenser for robust, accurate nanoliter‑to‑milliliter reagent dispensing across genomics and assay development. ([sptlabtech.com](https://www.sptlabtech.com/products/dragonfly/dragonfly-discovery?utm_source=openai))\n",
      "- dragonfly crystal: liquid handler optimized for protein crystallization hit optimization and gradient screen preparation, including viscous liquids, to accelerate structure‑based research. ([sptlabtech.com](https://www.sptlabtech.com/products/dragonfly/dragonfly-crystal?utm_source=openai))\n",
      "- apricot range (S3, S1, PP5, DC1): accessible 96/384‑channel automated pipetting systems for general liquid‑handling tasks; compatible with SPT‑branded EZ‑load and AL tips. ([sptlabtech.com](https://www.sptlabtech.com/products/apricot))\n",
      "- chameleon: next‑generation cryo‑EM sample preparation instrument enabling blot‑free vitrification and automated grid handling to improve throughput and data quality. ([sptlabtech.com](https://www.sptlabtech.com/products/chameleon?utm_source=openai))\n",
      "- Quantifoil cryo‑EM supports (UltrAuFoil, HexAuFoil, QUANTIFOIL Holey Carbon, SiO2 films, UTC, Quantifoil Active): market‑leading TEM grid supports, including self‑wicking grids co‑developed for exclusive use with chameleon, to enhance image quality and efficiency in cryo‑EM. ([sptlabtech.com](https://www.sptlabtech.com/products/quantifoil?utm_source=openai))\n",
      "- arktic (-20°C/-80°C) and arktic XC: automated biobanking stores providing compact, secure tube‑based storage with pneumatic sample transport and high‑density capacity. ([sptlabtech.com](https://www.sptlabtech.com/products/arktic?utm_source=openai))\n",
      "- comPOUND: scalable automated compound/sample store for -20°C, +4°C and ambient, using a carousel system for rapid “vending machine” retrieval of 2D‑barcoded tubes. ([sptlabtech.com](https://www.sptlabtech.com/products/compound?utm_source=openai))\n",
      "- comPACT: smaller‑footprint automated -20°C/+4°C store delivering vending‑style retrieval and modular scalability using the Group’s pneumatic transport technology. ([sptlabtech.com](https://www.sptlabtech.com/products/compact))\n",
      "- lab2lab: a pneumatic sample transport and scheduling network that moves 2D‑barcoded vials between labs, automated stores and analytics (e.g., LC/MS, NMR), integrating with instruments and LIMS. ([sptlabtech.com](https://www.sptlabtech.com/products/lab2lab?utm_source=openai))\n",
      "- BioMicroLab sample‑management instruments (XL9/XL20/XL100/XL200, LabelPro, VolumeCheck VC100/VC384, Scan/Scan+): benchtop systems for tube/vial handling, labeling, volume detection and high‑speed 1D/2D barcode decoding to streamline sample processing. ([sptlabtech.com](https://www.sptlabtech.com/products?utm_source=openai))\n",
      "- Consumables portfolio: apricot EZ‑load and AL pipette tips, dragonfly disposable syringes, firefly tips/reservoirs and crystallography plates, supporting the installed base of instruments. ([sptlabtech.com](https://www.sptlabtech.com/products/apricot/consumables?utm_source=openai))\n",
      "- Post‑sale service and support (“reliance” service): dedicated technical support to maintain instrument uptime and user productivity. ([sptlabtech.com](https://www.sptlabtech.com/products))\n",
      "\n",
      "Context from FY24 Directors’ Report: the Group’s principal activity is the research, development, manufacture and sale of instruments and consumables for automated liquid handling, sample preparation and sample management to accelerate life science research. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf))\n",
      "\n",
      "2 - SOURCES\n",
      "\n",
      "Sources:\n",
      "- Seaport Topco Limited – Group accounts FY24 (filed 30 Sep-25): Directors’ Report (principal activity). ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf))\n",
      "- Seaport Topco Limited – Group Strategic Report FY24 (operational KPIs context). ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf))\n",
      "- SPT Labtech – Products overview (portfolio map: firefly, mosquito, dragonfly, apricot, chameleon, Quantifoil, arktic, comPOUND, BioMicroLab, lab2lab, comPACT). ([sptlabtech.com](https://www.sptlabtech.com/products))\n",
      "- firefly product page. ([sptlabtech.com](https://www.sptlabtech.com/products/firefly?utm_source=openai))\n",
      "- mosquito product family page. ([sptlabtech.com](https://www.sptlabtech.com/products/mosquito?utm_source=openai))\n",
      "- dragonfly discovery product page. ([sptlabtech.com](https://www.sptlabtech.com/products/dragonfly/dragonfly-discovery?utm_source=openai))\n",
      "- dragonfly crystal product page. ([sptlabtech.com](https://www.sptlabtech.com/products/dragonfly/dragonfly-crystal?utm_source=openai))\n",
      "- apricot product family page; apricot tips and consumables page. ([sptlabtech.com](https://www.sptlabtech.com/products/apricot))\n",
      "- chameleon product page. ([sptlabtech.com](https://www.sptlabtech.com/products/chameleon?utm_source=openai))\n",
      "- Quantifoil product family pages (portfolio and self‑wicking grids). ([sptlabtech.com](https://www.sptlabtech.com/products/quantifoil?utm_source=openai))\n",
      "- arktic product pages (arktic, arktic XC) and automated biobanking overview. ([sptlabtech.com](https://www.sptlabtech.com/products/arktic?utm_source=openai))\n",
      "- comPOUND product page. ([sptlabtech.com](https://www.sptlabtech.com/products/compound?utm_source=openai))\n",
      "- comPACT product page. ([sptlabtech.com](https://www.sptlabtech.com/products/compact))\n",
      "- lab2lab product page. ([sptlabtech.com](https://www.sptlabtech.com/products/lab2lab?utm_source=openai))\n",
      "- BioMicroLab portfolio overview and representative product pages (XL200, VC100/VC384, Scan/Scan+). ([sptlabtech.com](https://www.sptlabtech.com/products?utm_source=openai))\n"
     ]
    }
   ],
   "source": [
    "prod_serv_ov = agent._generate_section('GENERATE PRODUCTS SERVICES OVERVIEW')\n",
    "print(prod_serv_ov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b39482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4b. Geographical Footprint\n",
      "\n",
      "| Country | Office (HQ/other) | Manufacturing facility | Sales/commercial office | Notes (evidence) |\n",
      "| --- | --- | --- | --- | --- |\n",
      "| United Kingdom | Yes (HQ) | Yes | Not disclosed | Melbourn (Cambridge) is the global R&D and production centre for the core SPT Labtech range; Seaport Topco’s and key UK subsidiaries’ registered office: Building F, Melbourn Science Park, Cambridge Road, Melbourn SG8 6HB. ([sptlabtech.com](https://www.sptlabtech.com/company)) |\n",
      "| United States | Yes | Yes | Not disclosed | Covina, CA is the production centre for Apricot Designs and BioMicroLab ranges; group subsidiaries include Apricot Designs, Inc. (677 Arrow Grand Circle, Covina, CA 91722) and SPT Labtech, Inc. (One Boston Place, Suite 26000, Boston, MA 02108). ([sptlabtech.com](https://www.sptlabtech.com/company)) |\n",
      "| Germany | Yes | Yes | Not disclosed | Quantifoil MicroTools GmbH site in Jena is the global R&D and production centre for Quantifoil; address: In den Brückenäckern 4, 07751 Großlöbichau, Jena. ([sptlabtech.com](https://www.sptlabtech.com/company)) |\n",
      "| China | Yes | Yes | Yes | Hangzhou is listed as a production centre; Shanghai is listed as a commercial hub. Subsidiaries include Apricot Designs (Shanghai) Co. Ltd.; Hangzhou Felogix Instruments Co. Ltd.; Shanghai Apricot Laboratory Instruments Co. Ltd. (addresses in Shanghai and Hangzhou). ([sptlabtech.com](https://www.sptlabtech.com/company)) |\n",
      "| Japan | Yes | No | Yes | Tokyo is listed as a commercial hub; subsidiary SPT Life Sciences Japan Kabushiki Kaisha registered at Center Building 527, Tokyo Ryutsu Center, Heiwajima 6-1-1, Ota-ku, Tokyo. ([sptlabtech.com](https://www.sptlabtech.com/company)) |\n",
      "| India | Yes | No | Yes | Delhi is listed as a commercial hub; subsidiary SPT Labtech India Pvt Ltd registered at SRS Tower, 134-135, First Floor, Sector 31, 14/15 Mathura Road, Faridabad 121003 (Delhi NCR). ([sptlabtech.com](https://www.sptlabtech.com/company)) |\n",
      "| Luxembourg | Yes | No | Not disclosed | SPT Labtech S.à r.l. incorporated with registered office at 55, Rue de Luxembourg, L-8077 Bertrange; no operational role stated. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf)) |\n",
      "| Hong Kong | Yes | No | Not disclosed | Group subsidiaries registered at Unit 417, 4/F, Lippo Centre, Tower Two, 89 Queensway, Admiralty, and Room 803, 8/F, Easey Commercial Building, 253–261 Hennessy Road, Wan Chai; roles not described as manufacturing. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf)) |\n",
      "| Cayman Islands | Yes | No | Not disclosed | Meadowcroft Enterprises Limited registered c/o Vistra (Cayman) Limited, PO Box 31119, Grand Pavilion, Hibiscus Way, 802 West Bay Road, Grand Cayman KY1-1205; appears to be a holding/administrative entity. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf)) |\n",
      "| British Virgin Islands | Yes | No | Not disclosed | LBD ESOP Ventures Limited and Bio-Direct Ventures Limited registered at Trinity Chambers, P.O. Box 4301, Road Town, Tortola; appear to be holding/ESOP vehicles. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf)) |\n",
      "\n",
      "Notes:\n",
      "- “Not disclosed” indicates the specific function (e.g., dedicated sales office) is not explicitly stated on the official website or in the latest filed annual report; entries reflect registered entities/addresses only. Where the SPT Labtech website labels a location as “Production centre” or “Commercial hub,” that is shown accordingly. ([sptlabtech.com](https://www.sptlabtech.com/company))\n",
      "- Seaport Topco Limited is the holding company for the SPT Labtech group, which “trades collectively as SPT Labtech,” hence the use of the group’s official site for operating footprint, complemented by subsidiary registers in the FY24 group accounts filed on 30-Sep-25. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history/MzQ4MjkwMzE0OWFkaXF6a2N4/document?download=0&format=pdf))\n"
     ]
    }
   ],
   "source": [
    "geo_foot = agent._generate_section('GENERATE GEO FOOTPRINT')\n",
    "\n",
    "print(geo_foot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdf3dc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. Key Developments\n",
      "\n",
      "- Oct-25: Seaport Topco Limited, through operating company SPT Labtech, announced a strategic partnership with 10x Genomics to provide automated single‑cell workflows on the firefly platform, with future expansion planned for Visium Spatial. ([sptlabtech.com](https://www.sptlabtech.com/news/spt-labtech-partners-with-10x?utm_source=openai))\n",
      "- Jul-25: Seaport Topco Limited, via SPT Labtech, entered a collaboration with Semarion to integrate the firefly liquid handling platform with SemaCyte microcarriers to advance automated, miniaturized cell‑based assay workflows. ([sptlabtech.com](https://www.sptlabtech.com/news/spt-labtech-and-semarion-collaborate-to-advance-automated-cell-based-assay-workflows?utm_source=openai))\n",
      "- Jun-25: Seaport Topco Limited reported that SPT Labtech was named an Illumina Qualified Methods Provider after qualifying an automated Illumina DNA Prep workflow on the firefly platform. ([sptlabtech.com](https://www.sptlabtech.com/news/spt-labtech-named-illumina-qualified-methods-provider-following-successful-automation-of-illumina-dna-prep-on-firefly-platform?utm_source=openai))\n",
      "- Mar-25: Seaport Topco Limited recorded board changes at Companies House, filing the termination of Roland Diggelmann as a director effective Dec-24, Paula Lynn Dowdy effective Feb-25, and Robert Friel effective Mar-25. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history))\n",
      "- Mar-25: Seaport Topco Limited, through SPT Labtech, launched a Joint Laboratory of Structural Biology with Biortus in Wuxi to accelerate structure‑based drug discovery using chameleon and cryo‑EM capabilities. ([sptlabtech.com](https://www.sptlabtech.com/news/spt-labtech-and-biortus-launch-joint-laboratory-of-structural-biology-in-china?utm_source=openai))\n",
      "- Jan-25: Seaport Topco Limited filed a statement of capital reflecting an allotment of shares dated Nov-24, indicating additional share issuance at the holding‑company level. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history))\n",
      "- Oct-24: Seaport Topco Limited, via SPT Labtech, and ICE Bioscience launched a joint automated laboratory in Beijing focused on intelligent drug screening and life‑sciences automation. ([sptlabtech.com](https://www.sptlabtech.com/news/spt-labtech-and-ice-bioscience-launch-joint-automated-laboratory?utm_source=openai))\n",
      "- Jul-24: Seaport Topco Limited appointed Robert Hugh Walton as a director, aligning with his leadership transition to become SPT Labtech’s Chief Executive Officer effective May-24. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history))\n",
      "- Mar-24: Seaport Topco Limited adopted new Articles of Association, varied share rights and recorded a share allotment as part of a corporate reorganization at the parent‑company level. ([find-and-update.company-information.service.gov.uk](https://find-and-update.company-information.service.gov.uk/company/14171962/filing-history))\n",
      "- Feb-24: Seaport Topco Limited’s operating company SPT Labtech appointed Morten Frost Norgreen as Chief Commercial Officer to strengthen global commercial execution. ([bio-itworld.com](https://www.bio-itworld.com/pressreleases/2024/02/05/spt-labtech-appoints-new-chief-commercial-officer?utm_source=openai))\n"
     ]
    }
   ],
   "source": [
    "dev_high = agent._generate_section('GENERATE DEVELOPMENTS HIGHLIGHTS')\n",
    "print(dev_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d495a60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "def markdown_table_to_docx(markdown_text: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Convert markdown text with MULTIPLE tables to a Word document.\n",
    "    Handles multiple tables with different column counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = markdown_text.strip().split('\\n')\n",
    "    \n",
    "    # Create document\n",
    "    doc = Document()\n",
    "    \n",
    "    # Process line by line\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Detect table start\n",
    "        if line.startswith('|'):\n",
    "            # Collect all consecutive table lines\n",
    "            table_lines = []\n",
    "            while i < len(lines) and lines[i].strip().startswith('|'):\n",
    "                line = lines[i].strip()\n",
    "                if '---' not in line:  # Skip separator lines\n",
    "                    table_lines.append(line)\n",
    "                i += 1\n",
    "            \n",
    "            # Create table in Word\n",
    "            if table_lines:\n",
    "                rows = []\n",
    "                for line in table_lines:\n",
    "                    cells = [c.strip() for c in line.strip('|').split('|')]\n",
    "                    rows.append(cells)\n",
    "                \n",
    "                # Ensure all rows have same number of columns\n",
    "                if rows:\n",
    "                    max_cols = max(len(row) for row in rows)\n",
    "                    \n",
    "                    # Pad rows with fewer columns\n",
    "                    for row in rows:\n",
    "                        while len(row) < max_cols:\n",
    "                            row.append('')\n",
    "                    \n",
    "                    # Create Word table\n",
    "                    table = doc.add_table(rows=len(rows), cols=max_cols)\n",
    "                    table.style = 'Light Grid Accent 1'\n",
    "                    \n",
    "                    # Populate table\n",
    "                    for row_idx, row_data in enumerate(rows):\n",
    "                        for col_idx, cell_data in enumerate(row_data):\n",
    "                            if col_idx < len(table.rows[row_idx].cells):\n",
    "                                cell = table.rows[row_idx].cells[col_idx]\n",
    "                                cell.text = cell_data\n",
    "                                \n",
    "                                # Style header row\n",
    "                                if row_idx == 0:\n",
    "                                    for paragraph in cell.paragraphs:\n",
    "                                        for run in paragraph.runs:\n",
    "                                            run.bold = True\n",
    "                    \n",
    "                    # Add spacing after table\n",
    "                    doc.add_paragraph()\n",
    "        \n",
    "        # Handle headings (Summary / Interpretation, Sources, etc.)\n",
    "        elif line in [\n",
    "            '1. Business Overview', '2. Key Stakeholders', '3. Revenue Split', '4a. Products/Services Overview', '4b. Geographical Footprint',\n",
    "            '5. Key Developments', '6. Financial Highlights', '7. Capital Structure']:\n",
    "            para = doc.add_paragraph(line)\n",
    "            para.runs[0].bold = True\n",
    "            para.runs[0].font.size = Pt(16)\n",
    "            i += 1\n",
    "\n",
    "        elif line in [\n",
    "            'Summary / Interpretation', 'Sources:', 'Sources']:\n",
    "            para = doc.add_paragraph(line)\n",
    "            para.runs[0].bold = True\n",
    "            para.runs[0].font.size = Pt(12)\n",
    "            i += 1\n",
    "        \n",
    "        # Handle bullet points\n",
    "        elif line.startswith('-') or line.startswith('•'):\n",
    "            para = doc.add_paragraph(line[1:].strip(), style='List Bullet')\n",
    "            i += 1\n",
    "        \n",
    "        # Handle regular paragraphs\n",
    "        elif line:\n",
    "            doc.add_paragraph(line)\n",
    "            i += 1\n",
    "        \n",
    "        # Skip empty lines\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Save\n",
    "    doc.save(output_path)\n",
    "    print(f\"✓ Saved to: {output_path}\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8153f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to: output_from_markdown.docx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0x12989a2b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markdown_table_to_docx(biz_ov, \"output_from_markdown.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfb7a038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to: output_from_markdown.docx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0x12989ab50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all = \"\\n\\n\".join([biz_ov])\n",
    "# biz_ov + key_stake + fin_high + cap_stru + rev_split + prod_serv_ov + geo_foot + dev_high\n",
    "\n",
    "all = \"\\n\\n\".join([biz_ov, key_stake, rev_split, prod_serv_ov,geo_foot, dev_high, fin_high, cap_stru])\n",
    "\n",
    "\n",
    "markdown_table_to_docx(all, \"output_from_markdown.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75fff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved to: output.docx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<docx.document.Document at 0x12a7b9ca0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "from docx import Document\n",
    "from docx.shared import Pt, RGBColor, Inches, Cm\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "\n",
    "def markdown_table_to_docx(markdown_text: str, output_path: str, logo_path: str = None):\n",
    "    \"\"\"\n",
    "    Convert markdown to Docx with a logo positioned at top-left.\n",
    "    - Left: -3cm indent\n",
    "    - Height: Moved up by reducing header distance to 0.5cm\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = markdown_text.strip().split('\\n')\n",
    "    doc = Document()\n",
    "    \n",
    "    # --- LOGO POSITIONING START ---\n",
    "    if logo_path:\n",
    "        section = doc.sections[0]\n",
    "        header = section.header\n",
    "        header_para = header.paragraphs[0]\n",
    "        \n",
    "        # 1. VERTICAL POSITION (Height)\n",
    "        # \"Header distance\" is the gap from the top edge of the paper to the start of the header.\n",
    "        # Default is usually ~1.27cm. Setting it to 0.5cm moves the logo UP.\n",
    "        section.header_distance = Cm(0.5)\n",
    "        \n",
    "        # 2. HORIZONTAL POSITION (Left)\n",
    "        # Align left and use negative indent to pull it into the margin.\n",
    "        header_para.alignment = WD_ALIGN_PARAGRAPH.LEFT\n",
    "        header_para.paragraph_format.left_indent = Cm(-3)\n",
    "        \n",
    "        # Remove any extra spacing that might push it down\n",
    "        header_para.paragraph_format.space_before = Pt(0)\n",
    "        header_para.paragraph_format.space_after = Pt(0)\n",
    "        \n",
    "        # 3. INSERT IMAGE\n",
    "        run = header_para.add_run()\n",
    "        try:\n",
    "            # Adjust width as needed\n",
    "            run.add_picture(logo_path, width=Inches(1))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Logo file not found at {logo_path}\")\n",
    "    # --- LOGO POSITIONING END ---\n",
    "    \n",
    "    # Process text content\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "        \n",
    "        # Detect table start\n",
    "        if line.startswith('|'):\n",
    "            table_lines = []\n",
    "            while i < len(lines) and lines[i].strip().startswith('|'):\n",
    "                line = lines[i].strip()\n",
    "                if '---' not in line:\n",
    "                    table_lines.append(line)\n",
    "                i += 1\n",
    "            \n",
    "            if table_lines:\n",
    "                rows = []\n",
    "                for line in table_lines:\n",
    "                    cells = [c.strip() for c in line.strip('|').split('|')]\n",
    "                    rows.append(cells)\n",
    "                \n",
    "                if rows:\n",
    "                    max_cols = max(len(row) for row in rows)\n",
    "                    \n",
    "                    # Pad rows\n",
    "                    for row in rows:\n",
    "                        while len(row) < max_cols:\n",
    "                            row.append('')\n",
    "                    \n",
    "                    table = doc.add_table(rows=len(rows), cols=max_cols)\n",
    "                    table.style = 'Light Grid Accent 1'\n",
    "                    \n",
    "                    for row_idx, row_data in enumerate(rows):\n",
    "                        for col_idx, cell_data in enumerate(row_data):\n",
    "                            if col_idx < len(table.rows[row_idx].cells):\n",
    "                                cell = table.rows[row_idx].cells[col_idx]\n",
    "                                cell.text = cell_data\n",
    "                                if row_idx == 0:\n",
    "                                    for paragraph in cell.paragraphs:\n",
    "                                        for run in paragraph.runs:\n",
    "                                            run.bold = True\n",
    "                    doc.add_paragraph()\n",
    "        \n",
    "        # Headings\n",
    "        elif line in [\n",
    "            '1. Business Overview', '2. Key Stakeholders', '3. Revenue Split', \n",
    "            '4a. Products/Services Overview', '4b. Geographical Footprint',\n",
    "            '5. Key Developments', '6. Financial Highlights', '7. Capital Structure']:\n",
    "            para = doc.add_paragraph(line)\n",
    "            para.runs[0].bold = True\n",
    "            para.runs[0].font.size = Pt(16)\n",
    "            i += 1\n",
    "\n",
    "        elif line in ['Summary / Interpretation', 'Sources:', 'Sources']:\n",
    "            para = doc.add_paragraph(line)\n",
    "            para.runs[0].bold = True\n",
    "            para.runs[0].font.size = Pt(12)\n",
    "            i += 1\n",
    "        \n",
    "        # Bullet points\n",
    "        elif line.startswith('-') or line.startswith('•'):\n",
    "            doc.add_paragraph(line[1:].strip(), style='List Bullet')\n",
    "            i += 1\n",
    "        \n",
    "        # Regular paragraphs\n",
    "        elif line:\n",
    "            doc.add_paragraph(line)\n",
    "            i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    doc.save(output_path)\n",
    "    print(f\"✓ Saved to: {output_path}\")\n",
    "    return doc\n",
    "markdown_table_to_docx(\n",
    "    all, \n",
    "    \"output.docx\", \n",
    "    logo_path=\"logo_teneo.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b79f39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_BREAK\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "def insert_table_data_generic(\n",
    "    gpt_output: str,\n",
    "    doc_path: str,\n",
    "    table_type: str,\n",
    "    metric_mapping: Optional[Dict[str, str]] = None,\n",
    "    doc: Optional[Document] = None\n",
    ") -> Document:\n",
    "    \"\"\"\n",
    "    Generic function to insert GPT-generated data into docx tables.\n",
    "    \n",
    "    Args:\n",
    "        gpt_output: The GPT response containing CSV/Table data and summary\n",
    "        doc_path: Path to the docx file\n",
    "        table_type: Type of table - 'capital_structure', 'financial_performance', or 'key_stakeholders'\n",
    "        metric_mapping: Optional dict to map CSV metric names to template row labels\n",
    "                       Keys should be normalized (lowercase, no special chars)\n",
    "        doc: Optional existing Document object. If None, will load from doc_path\n",
    "    \n",
    "    Returns:\n",
    "        Updated Document object\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================\n",
    "    # Helper functions\n",
    "    # =========================\n",
    "    def norm(s: str) -> str:\n",
    "        \"\"\"Normalize string for comparison (lowercase, alphanumeric only)\"\"\"\n",
    "        return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "    \n",
    "    def keynorm(s: str) -> str:\n",
    "        \"\"\"Normalize string for dictionary keys\"\"\"\n",
    "        return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "    \n",
    "    def tokens(s: str) -> set:\n",
    "        return set(re.findall(r\"[a-z0-9]+\", (s or \"\").lower()))\n",
    "    \n",
    "    def jaccard(a: str, b: str) -> float:\n",
    "        \"\"\"Calculate Jaccard similarity between two strings\"\"\"\n",
    "        ta, tb = tokens(a), tokens(b)\n",
    "        if not ta or not tb:\n",
    "            return 0.0\n",
    "        inter = len(ta & tb)\n",
    "        union = len(ta | tb)\n",
    "        return inter / union if union else 0.0\n",
    "    \n",
    "    def set_cell_text(cell, text: str):\n",
    "        \"\"\"Set cell text preserving formatting\"\"\"\n",
    "        if not cell.paragraphs:\n",
    "            cell.add_paragraph(text)\n",
    "            return\n",
    "        p = cell.paragraphs[0]\n",
    "        for run in p.runs:\n",
    "            run.text = \"\"\n",
    "        p.add_run(text)\n",
    "    \n",
    "    def set_paragraph_multiline(paragraph, text: str):\n",
    "        \"\"\"Replace a paragraph's text with multi-line content, preserving line breaks.\"\"\"\n",
    "        for run in paragraph.runs:\n",
    "            run.text = \"\"\n",
    "        lines = (text or \"\").splitlines()\n",
    "        if not lines:\n",
    "            return\n",
    "        paragraph.add_run(lines[0])\n",
    "        for ln in lines[1:]:\n",
    "            r = paragraph.add_run()\n",
    "            r.add_break(WD_BREAK.LINE)\n",
    "            paragraph.add_run(ln)\n",
    "    \n",
    "    # =========================\n",
    "    # 1) Extract CSV/Table + Summary from GPT output\n",
    "    # =========================\n",
    "    parts = gpt_output.split(\"\\n\\nSummary / Interpretation\", 1)\n",
    "    csv_block = parts[0].strip()\n",
    "    \n",
    "    # Find where the table/CSV starts\n",
    "    start = csv_block.find(\"Metric,\")\n",
    "    if start == -1:\n",
    "        # Try to find markdown table format\n",
    "        start = csv_block.find(\"| Metric |\")\n",
    "        if start != -1:\n",
    "            # Convert markdown table to CSV\n",
    "            lines = csv_block[start:].split(\"\\n\")\n",
    "            csv_lines = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"|\") and \"---\" not in line:\n",
    "                    # Remove leading/trailing pipes and split\n",
    "                    cells = [c.strip() for c in line.strip(\"|\").split(\"|\")]\n",
    "                    csv_lines.append(\",\".join(f'\"{c}\"' if \",\" in c else c for c in cells))\n",
    "                elif not line.startswith(\"|\"):\n",
    "                    break\n",
    "            csv_block = \"\\n\".join(csv_lines)\n",
    "        else:\n",
    "            raise ValueError(\"CSV/Table header 'Metric,' not found in model output.\")\n",
    "    else:\n",
    "        csv_block = csv_block[start:]\n",
    "    \n",
    "    summary_text = \"\"\n",
    "    if len(parts) > 1:\n",
    "        summary_text = \"Summary / Interpretation\" + parts[1].rstrip()\n",
    "    \n",
    "    # =========================\n",
    "    # 2) Parse CSV to DataFrame\n",
    "    # =========================\n",
    "    df = pd.read_csv(io.StringIO(csv_block))\n",
    "    \n",
    "    # Determine expected columns based on table type\n",
    "    if table_type == \"financial_performance\":\n",
    "        expected_cols = {\"Metric\", \"FY24\", \"FY23\", \"FY22\"}\n",
    "        year_cols = [\"FY24\", \"FY23\", \"FY22\"]\n",
    "    elif table_type == \"capital_structure\":\n",
    "        expected_cols = {\"Metric\", \"FY24\"}\n",
    "        year_cols = [\"FY24\"]\n",
    "    elif table_type == \"key_stakeholders\":\n",
    "        expected_cols = {\"Metric\", \"Shareholders\"}\n",
    "        year_cols = [\"Shareholders\"]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown table_type: {table_type}\")\n",
    "    \n",
    "    if not expected_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"{table_type} CSV columns missing. Expected {expected_cols}, Got: {list(df.columns)}\")\n",
    "    \n",
    "    # Create dict: normalized metric name -> values\n",
    "    if table_type in [\"financial_performance\", \"capital_structure\"]:\n",
    "        csv_rows = {\n",
    "            keynorm(str(df.at[i, \"Metric\"]).strip()): {\n",
    "                col: str(df.at[i, col]) for col in year_cols\n",
    "            }\n",
    "            for i in range(len(df))\n",
    "        }\n",
    "    else:  # key_stakeholders\n",
    "        csv_rows = {\n",
    "            keynorm(str(df.at[i, \"Metric\"]).strip()): str(df.at[i, \"Shareholders\"]).strip()\n",
    "            for i in range(len(df))\n",
    "        }\n",
    "    \n",
    "    # =========================\n",
    "    # 3) Open DOCX and find the target table\n",
    "    # =========================\n",
    "    if doc is None:\n",
    "        doc = Document(doc_path)\n",
    "    \n",
    "    def find_table_by_type(document: Document, ttype: str):\n",
    "        \"\"\"Find table based on type\"\"\"\n",
    "        if ttype == \"financial_performance\":\n",
    "            # Look for table with FY24/FY23/FY22 headers\n",
    "            for tbl in document.tables:\n",
    "                if len(tbl.rows):\n",
    "                    header = \" \".join(c.text for c in tbl.rows[0].cells)\n",
    "                    if all(x in norm(header) for x in [\"fy24\", \"fy23\", \"fy22\"]):\n",
    "                        return tbl\n",
    "            # Fallback: after \"Financial Performance\" heading\n",
    "            return find_table_after_heading(document, \"financialperformance\")\n",
    "        \n",
    "        elif ttype == \"capital_structure\":\n",
    "            # Look for table after \"Capital Structure\" heading\n",
    "            return find_table_after_heading(document, \"capitalstructure\")\n",
    "        \n",
    "        elif ttype == \"key_stakeholders\":\n",
    "            # Look for table with \"Title\" and \"Occupants\" columns\n",
    "            for tbl in document.tables:\n",
    "                if not tbl.rows:\n",
    "                    continue\n",
    "                header = [norm(c.text) for c in tbl.rows[0].cells]\n",
    "                if \"title\" in header and \"occupants\" in header:\n",
    "                    return tbl\n",
    "            # Fallback: after \"Key Stakeholders\" heading\n",
    "            return find_table_after_heading(document, \"keystakeholders\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def find_table_after_heading(document: Document, heading_normalized: str):\n",
    "        \"\"\"Find first table after a specific heading\"\"\"\n",
    "        found_heading = False\n",
    "        body = document._element.body\n",
    "        for child in body.iterchildren():\n",
    "            tag = child.tag.rsplit(\"}\", 1)[-1]\n",
    "            if tag == \"p\":\n",
    "                p_text = \"\".join(\n",
    "                    t.text for t in child.iter() if t.tag.rsplit(\"}\", 1)[-1] == \"t\"\n",
    "                ).strip()\n",
    "                if norm(p_text) == heading_normalized:\n",
    "                    found_heading = True\n",
    "            elif tag == \"tbl\" and found_heading:\n",
    "                from docx.table import Table\n",
    "                return Table(child, document)\n",
    "        return None\n",
    "    \n",
    "    table = find_table_by_type(doc, table_type)\n",
    "    if table is None:\n",
    "        raise RuntimeError(f\"Could not locate the '{table_type}' table.\")\n",
    "    \n",
    "    # =========================\n",
    "    # 4) Populate the table\n",
    "    # =========================\n",
    "    \n",
    "    # Detect column indices from header row\n",
    "    header_row = table.rows[0]\n",
    "    col_map = {}  # normalized header -> column index\n",
    "    for idx, cell in enumerate(header_row.cells):\n",
    "        col_map[norm(cell.text)] = idx\n",
    "    \n",
    "    # Build mapping for year columns\n",
    "    year_col_indices = {}\n",
    "    if table_type in [\"financial_performance\", \"capital_structure\"]:\n",
    "        for year in year_cols:\n",
    "            year_norm = norm(year)\n",
    "            if year_norm in col_map:\n",
    "                year_col_indices[year] = col_map[year_norm]\n",
    "    else:  # key_stakeholders\n",
    "        # Value column could be \"Shareholders\" or \"Occupants\"\n",
    "        value_col_idx = col_map.get(\"shareholders\") or col_map.get(\"occupants\")\n",
    "        if value_col_idx is None:\n",
    "            value_col_idx = 1  # Default to second column\n",
    "    \n",
    "    # Populate data rows\n",
    "    for row_idx in range(1, len(table.rows)):\n",
    "        row = table.rows[row_idx]\n",
    "        label_cell = row.cells[0]\n",
    "        label_text = label_cell.text.strip()\n",
    "        label_norm = keynorm(label_text)\n",
    "        \n",
    "        # Try direct match first\n",
    "        matched_key = None\n",
    "        if label_norm in csv_rows:\n",
    "            matched_key = label_norm\n",
    "        elif metric_mapping and label_norm in metric_mapping:\n",
    "            # Use provided mapping\n",
    "            mapped_key = keynorm(metric_mapping[label_norm])\n",
    "            if mapped_key in csv_rows:\n",
    "                matched_key = mapped_key\n",
    "        else:\n",
    "            # Try fuzzy matching with Jaccard similarity\n",
    "            best_score = 0.0\n",
    "            for csv_key in csv_rows.keys():\n",
    "                score = jaccard(label_norm, csv_key)\n",
    "                if score > best_score and score >= 0.6:  # Threshold\n",
    "                    best_score = score\n",
    "                    matched_key = csv_key\n",
    "        \n",
    "        # Populate cells if we found a match\n",
    "        if matched_key:\n",
    "            if table_type in [\"financial_performance\", \"capital_structure\"]:\n",
    "                for year, col_idx in year_col_indices.items():\n",
    "                    if col_idx < len(row.cells):\n",
    "                        value = csv_rows[matched_key].get(year, \"\")\n",
    "                        set_cell_text(row.cells[col_idx], str(value))\n",
    "            else:  # key_stakeholders\n",
    "                if value_col_idx < len(row.cells):\n",
    "                    value = csv_rows[matched_key]\n",
    "                    set_cell_text(row.cells[value_col_idx], str(value))\n",
    "    \n",
    "    # =========================\n",
    "    # 5) Insert Summary below the table (if present)\n",
    "    # =========================\n",
    "    if summary_text:\n",
    "        # Find the table in the document body and add summary after it\n",
    "        table_elem = table._element\n",
    "        parent = table_elem.getparent()\n",
    "        table_idx = list(parent).index(table_elem)\n",
    "        \n",
    "        # Look for existing summary paragraph after the table\n",
    "        summary_inserted = False\n",
    "        for i in range(table_idx + 1, len(parent)):\n",
    "            child = parent[i]\n",
    "            tag = child.tag.rsplit(\"}\", 1)[-1]\n",
    "            if tag == \"p\":\n",
    "                p_text = \"\".join(\n",
    "                    t.text for t in child.iter() if t.tag.rsplit(\"}\", 1)[-1] == \"t\"\n",
    "                ).strip()\n",
    "                if \"summary\" in norm(p_text) or \"interpretation\" in norm(p_text):\n",
    "                    # Found summary section - update it\n",
    "                    from docx.text.paragraph import Paragraph\n",
    "                    para = Paragraph(child, doc)\n",
    "                    set_paragraph_multiline(para, summary_text)\n",
    "                    summary_inserted = True\n",
    "                    break\n",
    "            elif tag == \"tbl\":\n",
    "                # Hit another table, stop looking\n",
    "                break\n",
    "        \n",
    "        # If no existing summary found, add new paragraph\n",
    "        if not summary_inserted:\n",
    "            # Add paragraph after table\n",
    "            new_para = doc.add_paragraph()\n",
    "            set_paragraph_multiline(new_para, summary_text)\n",
    "    \n",
    "    return doc\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Convenience wrapper functions\n",
    "# =========================\n",
    "\n",
    "def insert_capital_structure(gpt_output: str, doc_path: str = None, doc: Document = None) -> Document:\n",
    "    \"\"\"Insert capital structure data into docx table.\"\"\"\n",
    "    return insert_table_data_generic(\n",
    "        gpt_output=gpt_output,\n",
    "        doc_path=doc_path,\n",
    "        table_type=\"capital_structure\",\n",
    "        doc=doc\n",
    "    )\n",
    "\n",
    "\n",
    "def insert_finance(gpt_output: str, doc_path: str = None, doc: Document = None,\n",
    "                   metric_mapping: Optional[Dict[str, str]] = None) -> Document:\n",
    "    \"\"\"Insert financial performance data into docx table.\"\"\"\n",
    "    # Default metric mapping for financial performance\n",
    "    if metric_mapping is None:\n",
    "        metric_mapping = {\n",
    "            keynorm(\"Revenue (Turnover)\"): \"Revenue\",\n",
    "            keynorm(\"Revenue growth % (yoy)\"): \"Revenue Growth\",\n",
    "            keynorm(\"Gross profit\"): \"Gross Profit\",\n",
    "            keynorm(\"Gross margin %\"): \"Gross Margin\",\n",
    "            keynorm(\"EBITDA\"): \"EBITDA\",\n",
    "            keynorm(\"EBITDA margin %\"): \"EBITDA Margin\",\n",
    "            keynorm(\"Adjusted EBITDA\"): \"Adjusted EBITDA\",\n",
    "            keynorm(\"Capex (tangible+intangible)\"): \"CAPEX\",\n",
    "            keynorm(\"CFADS\"): \"CFADS\",\n",
    "            keynorm(\"Net working capital change\"): \"Net Working Capital Change\",\n",
    "            keynorm(\"Total debt\"): \"Total Debt\",\n",
    "            keynorm(\"Net debt\"): \"Net Debt\",\n",
    "            keynorm(\"Leverage (Net Debt/EBITDA)\"): \"Leverage\",\n",
    "        }\n",
    "    \n",
    "    return insert_table_data_generic(\n",
    "        gpt_output=gpt_output,\n",
    "        doc_path=doc_path,\n",
    "        table_type=\"financial_performance\",\n",
    "        metric_mapping=metric_mapping,\n",
    "        doc=doc\n",
    "    )\n",
    "\n",
    "\n",
    "def insert_stakeholders(gpt_output: str, doc_path: str = None, doc: Document = None) -> Document:\n",
    "    \"\"\"Insert key stakeholders data into docx table.\"\"\"\n",
    "    return insert_table_data_generic(\n",
    "        gpt_output=gpt_output,\n",
    "        doc_path=doc_path,\n",
    "        table_type=\"key_stakeholders\",\n",
    "        doc=doc\n",
    "    )\n",
    "\n",
    "\n",
    "# Helper to ensure keynorm is available\n",
    "def keynorm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62112952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_capital_structure_with_mapping(gpt_output: str, doc_path: str = None, doc: Document = None) -> Document:\n",
    "    \"\"\"\n",
    "    Insert capital structure with custom metric mapping for your template.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the metric mapping (template label -> CSV metric name)\n",
    "    metric_mapping = {\n",
    "        # Normalize both sides\n",
    "        keynorm(\"Facility Name\"): keynorm(\"Facility Name\"),\n",
    "        keynorm(\"Interest Rate\"): keynorm(\"Interest Rate\"),\n",
    "        keynorm(\"Maturity\"): keynorm(\"Maturity\"),\n",
    "        keynorm(\"Amount Outstanding\"): keynorm(\"Bank loans + RCF outstanding (excl. leases)\"),  # THIS IS KEY!\n",
    "        keynorm(\"Gross External Debt\"): keynorm(\"Bank loans + RCF outstanding (excl. leases)\"),\n",
    "        keynorm(\"Cash (Closing Cash)\"): keynorm(\"Cash (Closing Cash)\"),\n",
    "        keynorm(\"Net External Debt\"): keynorm(\"Net Debt\"),\n",
    "        keynorm(\"Liquidity\"): keynorm(\"Liquidity\"),\n",
    "        keynorm(\"EBITDA\"): keynorm(\"Adjusted EBITDA\"),\n",
    "        keynorm(\"Leverage\"): keynorm(\"Leverage (Net Debt/EBITDA)\"),\n",
    "    }\n",
    "    \n",
    "    # Parse the markdown table from gpt_output\n",
    "    import io, re\n",
    "    import pandas as pd\n",
    "    from docx import Document\n",
    "    \n",
    "    def norm(s: str) -> str:\n",
    "        return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "    \n",
    "    def keynorm(s: str) -> str:\n",
    "        return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "    \n",
    "    def set_cell_text(cell, text: str):\n",
    "        if not cell.paragraphs:\n",
    "            cell.add_paragraph(text)\n",
    "            return\n",
    "        p = cell.paragraphs[0]\n",
    "        for run in p.runs:\n",
    "            run.text = \"\"\n",
    "        p.add_run(text)\n",
    "    \n",
    "    # Extract markdown table\n",
    "    gpt_output = gpt_output.strip()\n",
    "    lines = gpt_output.split('\\n')\n",
    "    \n",
    "    # Find table boundaries\n",
    "    csv_lines = []\n",
    "    in_table = False\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith('| Metric |'):\n",
    "            in_table = True\n",
    "        if in_table and line.startswith('|') and '---' not in line:\n",
    "            cells = [c.strip() for c in line.strip('|').split('|')]\n",
    "            csv_lines.append(cells)\n",
    "        elif in_table and not line.startswith('|'):\n",
    "            break\n",
    "    \n",
    "    if len(csv_lines) < 2:\n",
    "        raise ValueError(\"Could not parse capital structure table\")\n",
    "    \n",
    "    # Convert to dict: normalized metric -> FY24 value\n",
    "    header = csv_lines[0]\n",
    "    data_rows = csv_lines[1:]\n",
    "    \n",
    "    csv_data = {}\n",
    "    for row in data_rows:\n",
    "        if len(row) >= 2:\n",
    "            metric = keynorm(row[0])\n",
    "            value_fy24 = row[1] if len(row) > 1 else \"n.a.\"\n",
    "            csv_data[metric] = value_fy24\n",
    "    \n",
    "    print(f\"DEBUG: Parsed {len(csv_data)} metrics from CSV\")\n",
    "    print(f\"CSV metrics: {list(csv_data.keys())[:5]}...\")\n",
    "    \n",
    "    # Load document\n",
    "    if doc is None:\n",
    "        doc = Document(doc_path)\n",
    "    \n",
    "    # Find capital structure table (Table 2 based on inspection)\n",
    "    table = None\n",
    "    for tbl in doc.tables:\n",
    "        if len(tbl.rows) > 5:\n",
    "            # Check if it has capital structure metrics\n",
    "            row_texts = [tbl.rows[i].cells[0].text.strip().lower() for i in range(min(5, len(tbl.rows)))]\n",
    "            if any('facility' in t or 'leverage' in t or 'ebitda' in t for t in row_texts):\n",
    "                table = tbl\n",
    "                break\n",
    "    \n",
    "    if table is None:\n",
    "        raise RuntimeError(\"Could not locate capital structure table\")\n",
    "    \n",
    "    print(f\"DEBUG: Found table with {len(table.rows)} rows\")\n",
    "    \n",
    "    # Populate table\n",
    "    for row_idx in range(1, len(table.rows)):  # Skip header\n",
    "        row = table.rows[row_idx]\n",
    "        label_text = row.cells[0].text.strip()\n",
    "        label_norm = keynorm(label_text)\n",
    "        \n",
    "        print(f\"DEBUG: Processing row '{label_text}' (normalized: '{label_norm}')\")\n",
    "        \n",
    "        # Try metric mapping first\n",
    "        matched_value = None\n",
    "        if label_norm in metric_mapping:\n",
    "            csv_metric = metric_mapping[label_norm]\n",
    "            print(f\"  Mapped to CSV metric: '{csv_metric}'\")\n",
    "            if csv_metric in csv_data:\n",
    "                matched_value = csv_data[csv_metric]\n",
    "                print(f\"  Found value: '{matched_value}'\")\n",
    "        \n",
    "        # Try direct match\n",
    "        if matched_value is None and label_norm in csv_data:\n",
    "            matched_value = csv_data[label_norm]\n",
    "            print(f\"  Direct match found: '{matched_value}'\")\n",
    "        \n",
    "        # Populate cell\n",
    "        if matched_value and len(row.cells) > 1:\n",
    "            set_cell_text(row.cells[1], matched_value)\n",
    "            print(f\"  ✓ Set cell value\")\n",
    "        else:\n",
    "            print(f\"  ✗ No match found\")\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d240c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing insert_stakeholders...\n",
      "✗ Error: local variable 'keynorm' referenced before assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/6k/jcthlrlx33v3xr_0cy_fm57c0000gn/T/ipykernel_79627/612376763.py\", line 15, in <module>\n",
      "    doc = insert_capital_structure_with_mapping(stakeholders_test_output, doc_path=doc_path)\n",
      "  File \"/var/folders/6k/jcthlrlx33v3xr_0cy_fm57c0000gn/T/ipykernel_79627/3181027580.py\", line 9, in insert_capital_structure_with_mapping\n",
      "    keynorm(\"Facility Name\"): keynorm(\"Facility Name\"),\n",
      "UnboundLocalError: local variable 'keynorm' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# TEST: insert_stakeholders\n",
    "# =========================\n",
    "\n",
    "# Sample GPT output (this is what your agent would return)\n",
    "stakeholders_test_output = cap_stru\n",
    "\n",
    "# Test the function\n",
    "print(\"Testing insert_stakeholders...\")\n",
    "try:\n",
    "    # Load your template document\n",
    "    doc_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile (1).docx\"\n",
    "    \n",
    "    # Call the function\n",
    "    doc = insert_capital_structure_with_mapping(stakeholders_test_output, doc_path=doc_path)\n",
    "    \n",
    "    # Save to a test output file\n",
    "    output_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/test_stakeholders.docx\"\n",
    "    doc.save(output_path)\n",
    "    \n",
    "    print(f\"✓ Success! Open the file to check: {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_content(gpt_output):\n",
    "\n",
    "    parts = gpt_output.split(\"\\n\\nSummary / Interpretation\", 1)\n",
    "    csv_block = parts[0].strip()\n",
    "\n",
    "    start = csv_block.find(\"Metric,\")\n",
    "    if start == -1:\n",
    "        raise ValueError(\"CSV header 'Metric,' not found in model output.\")\n",
    "    csv_block = csv_block[start:]\n",
    "    section_df = pd.read_csv(io.StringIO(csv_block))\n",
    "\n",
    "    summary_text = \"\"\n",
    "    if len(parts) > 1:\n",
    "        summary_text = \"Summary / Interpretation\" + parts[1].rstrip()\n",
    "\n",
    "    return summary_text, section_df\n",
    "\n",
    "def insert_paragraph(document: Document, placeholder: str, new_text: str):\n",
    "\n",
    "    def set_paragraph_multiline(paragraph, text: str):\n",
    "        \"\"\"Replace a paragraph's text with multi-line content, preserving line breaks.\"\"\"\n",
    "        # clear existing runs\n",
    "        for run in paragraph.runs:\n",
    "            run.text = \"\"\n",
    "        # write lines with explicit line breaks\n",
    "        lines = (text or \"\").splitlines()\n",
    "        if not lines:\n",
    "            return\n",
    "        paragraph.add_run(lines[0])\n",
    "        for ln in lines[1:]:\n",
    "            r = paragraph.add_run()\n",
    "            r.add_break(WD_BREAK.LINE)\n",
    "            paragraph.add_run(ln)\n",
    "\n",
    "    \"\"\"Find placeholder in paragraphs/cells and replace it with new_text (multiline).\"\"\"\n",
    "    # plain paragraphs\n",
    "    for p in document.paragraphs:\n",
    "        if placeholder in p.text:\n",
    "            set_paragraph_multiline(p, new_text)\n",
    "            return True\n",
    "    # inside tables\n",
    "    for tbl in document.tables:\n",
    "        for row in tbl.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    if placeholder in p.text:\n",
    "                        set_paragraph_multiline(p, new_text)\n",
    "                        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d708b6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
