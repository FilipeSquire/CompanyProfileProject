{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1ea4f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, textwrap\n",
    "import io\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "from xml.sax.saxutils import escape\n",
    "\n",
    "from gpts.gpt_assistants import general_assistant\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery\n",
    "from azure.core.exceptions import HttpResponseError\n",
    "from azure.search.documents.models import HybridSearch\n",
    "\n",
    "\n",
    "from openai import AzureOpenAI, APIConnectionError, OpenAI\n",
    "from prompts import new_system_finance_prompt\n",
    "\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "\n",
    "from prompts4 import finance_calculations, finance_pairs, capital_pairs, stakeholders_pairs, biz_overview_pairs, revenue_pairs, default_gpt_prompt, section4a, section4b, section5, section3\n",
    "from pages.design.func_tools import *\n",
    "from pages.design.formatting import *\n",
    "from pages.design.func_tools import docx_bytes_to_pdf_bytes\n",
    "import re, time\n",
    " \n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# ---- Config (expects the same envs you already used) ----\n",
    "SEARCH_ENDPOINT = os.environ[\"AZURE_SEARCH_ENDPOINT\"]\n",
    "SEARCH_INDEX    = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "SEARCH_KEY      = os.getenv(\"AZURE_SEARCH_API_KEY\")  # omit if using AAD/RBAC\n",
    "VECTOR_FIELD    = os.getenv(\"VECTOR_FIELD\")\n",
    "TEXT_FIELD      = os.getenv(\"TEXT_FIELD\")\n",
    "\n",
    "AOAI_ENDPOINT   = os.environ[\"AZURE_OPENAI_ENDPOINT\"]            # https://<resource>.openai.azure.com\n",
    "AOAI_API_VER    = os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"2024-10-21\")\n",
    "AOAI_DEPLOYMENT = os.environ[\"AZURE_OPENAI_DEPLOYMENT\"]          # e.g., gpt-4o-mini / o3-mini / gpt-5 preview\n",
    "AOAI_KEY        = os.getenv(\"AZURE_OPENAI_API_KEY\")              # omit if using AAD\n",
    "OPENAI_API_KEY  = os.getenv(\"OPENAI_API_KEY\")        # required\n",
    "\n",
    "# ------------------ CODE\n",
    "\n",
    "class profileAgent():\n",
    "\n",
    "    \"\"\"Hybrid (dense+sparse) RAG over Vector Store\n",
    "\n",
    "    This Agent is responsible for creating Company Profiles. \n",
    "    It operates with gpt5.\n",
    "    It is activated by a call on main rag when it is typed 'Create company profile'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, company_name, k, max_text_recall_size, max_chars, model, profile_prompt = new_system_finance_prompt, finance_calculations = finance_calculations):\n",
    "        \n",
    "        self.company_name = company_name\n",
    "\n",
    "        self.k = k\n",
    "        self.max_text_recall_size = max_text_recall_size\n",
    "        self.model = model\n",
    "        self.max_chars = max_chars\n",
    "\n",
    "        self.azure_credentials = AzureKeyCredential(SEARCH_KEY) if SEARCH_KEY else DefaultAzureCredential()\n",
    "        self.search_client = SearchClient(SEARCH_ENDPOINT, SEARCH_INDEX, credential=self.azure_credentials)\n",
    "\n",
    "        self.az_openai = AzureOpenAI(azure_endpoint=AOAI_ENDPOINT, api_key=AOAI_KEY, api_version=AOAI_API_VER)\n",
    "        self.profile_prompt = profile_prompt\n",
    "        self.web_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "        self.reasoning_effort = \"medium\"\n",
    "        self.verbosity = \"medium\"\n",
    "\n",
    "        self.finance_calculations = finance_calculations\n",
    "\n",
    "    def _company_filter(self) -> str:\n",
    "        v = (self.company_name or \"\").replace(\"'\", \"''\").strip()\n",
    "        return f\"company_name eq '{v}'\" if v else None\n",
    "    \n",
    "    def assemble_bm25_from_llm(self, slots: dict) -> str:\n",
    "        def q(s: str) -> str:\n",
    "            # sanitize: remove internal quotes and trim\n",
    "            s = (s or \"\").strip().replace('\"', ' ')\n",
    "            return f\"\\\"{s}\\\"\" if s else \"\"\n",
    "        groups = []\n",
    "\n",
    "        # must-have phrases (ANDed)\n",
    "        for p in slots.get(\"must_have_phrases\", []):\n",
    "            qp = q(p)\n",
    "            if qp:\n",
    "                groups.append(qp)\n",
    "\n",
    "        # metric / statement synonym groups (ORed within each group)\n",
    "        for key in [\"metric\", \"statement\"]:\n",
    "            syns = slots.get(\"synonyms\", {}).get(key, []) or slots.get(key, [])\n",
    "            syns = [q(s) for s in syns if s]\n",
    "            if syns:\n",
    "                groups.append(\"(\" + \" OR \".join(syns) + \")\")\n",
    "\n",
    "        return \" AND \".join(groups) if groups else \"\\\"financial statements\\\"\"\n",
    "\n",
    "\n",
    "    def bm25_creator(self, prompt):\n",
    "\n",
    "        instruction = (\n",
    "            \"Extract finance search slots for Azure AI Search. \"\n",
    "            \"Return strict JSON: {\\\"metric\\\":[], \\\"statement\\\":[], \\\"synonyms\\\":{}, \\\"must_have_phrases\\\":[]} \"\n",
    "            \"(include IFRS/US GAAP variants).\"\n",
    "        )\n",
    "        resp = general_assistant(instruction, prompt, OPENAI_API_KEY, 'gpt-4o')\n",
    "\n",
    "        try:\n",
    "            slots = getattr(resp, \"output_json\", None)\n",
    "            if slots is None:\n",
    "                import json\n",
    "                slots = json.loads(resp.output_text)\n",
    "        except Exception:\n",
    "            # fallback: minimal anchors from prompt\n",
    "            slots = {\"must_have_phrases\": [prompt], \"metric\": [], \"statement\": [], \"synonyms\": {}}\n",
    "        return self.assemble_bm25_from_llm(slots)\n",
    "\n",
    "    def _retrieve_hybrid_enhanced(self, query_nl, k: int = 50, top_n = 30, fields=VECTOR_FIELD, max_text_recall_size:int = 800):\n",
    "        sc = self.search_client\n",
    "        flt = self._company_filter()\n",
    "        \n",
    "        try:\n",
    "            vq = VectorizableTextQuery(text=query_nl, k=k, fields=VECTOR_FIELD)\n",
    "            # Prefer vector-only search (integrated vectorization). If your index isn't set up for it, this raises.\n",
    "            results = sc.search(\n",
    "                search_text=self.bm25_creator(query_nl), \n",
    "                vector_queries=[vq], \n",
    "                top=top_n, \n",
    "                query_type=\"semantic\",\n",
    "                query_caption=\"extractive\", \n",
    "                hybrid_search=HybridSearch(max_text_recall_size=self.max_text_recall_size),\n",
    "                query_caption_highlight_enabled=True,\n",
    "                filter=flt\n",
    "                )\n",
    "            mode = \"hybrid + semantic\"\n",
    "        except HttpResponseError as e:\n",
    "            # Fall back to lexical so you still get results while fixing vector config\n",
    "            results = sc.search(search_text=self.bm25_creator(query_nl), top=k)\n",
    "            mode = f\"lexical (fallback due to: {e.__class__.__name__})\"\n",
    "\n",
    "        hits: List[Dict] = []\n",
    "        for r in results:\n",
    "            d = r.copy() if hasattr(r, \"copy\") else {k2: r[k2] for k2 in r}\n",
    "            d[\"score\"] = d.get(\"@search.reranker_score\") or d.get(\"@search.score\") or 0.0\n",
    "            caps = d.get(\"@search.captions\")\n",
    "            if isinstance(caps, list) and caps:\n",
    "                d[\"caption\"] = getattr(caps[0], \"text\", None)\n",
    "            hits.append(d)\n",
    "\n",
    "        return mode, hits\n",
    "\n",
    "\n",
    "    def _build_context(self, hits: List[Dict], text_field: str = TEXT_FIELD, max_chars: int = 20000):\n",
    "        \"\"\"Build a compact, numbered context block and also return the selected chunk metadata.\"\"\"\n",
    "        lines = []\n",
    "        total = 0\n",
    "        selected = []  # <- we'll return this\n",
    "\n",
    "        for i, h in enumerate(hits, 1):\n",
    "            title     = h.get(\"title\")\n",
    "            chunk_id  = h.get(\"chunk_id\")\n",
    "            full_text = (h.get(text_field) or \"\")\n",
    "            if not full_text:\n",
    "                continue\n",
    "\n",
    "            preview = textwrap.shorten(full_text, width=700, placeholder=\" ...\")\n",
    "            block = f\"[{i}] title={title!r} | chunk_id={chunk_id} | score={h.get('score'):.4f}\\n{full_text}\"\n",
    "\n",
    "            if total + len(block) > self.max_chars:\n",
    "                break\n",
    "\n",
    "            total += len(block)\n",
    "            lines.append(block)\n",
    "\n",
    "            # keep rich metadata so you can show or log it later\n",
    "            selected.append({\n",
    "                \"i\": i,\n",
    "                \"title\": title,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"score\": h.get(\"score\"),\n",
    "                \"caption\": h.get(\"caption\"),\n",
    "                \"preview\": preview,\n",
    "                \"text\": full_text,  # full chunk text (not shortened)\n",
    "                # include any other fields you index, if available:\n",
    "                \"metadata_storage_path\": h.get(\"metadata_storage_path\"),\n",
    "                \"page_number\": h.get(\"page_number\"),\n",
    "                \"doc_type\": h.get(\"doc_type\"),\n",
    "            })\n",
    "\n",
    "        return \"\\n\\n---\\n\\n\".join(lines), selected\n",
    "\n",
    "        \n",
    "    def _generate_pdf(self, text: str) -> bytes:\n",
    "\n",
    "        buf = io.BytesIO()\n",
    "        doc = SimpleDocTemplate(buf, pagesize=letter)\n",
    "        styles = getSampleStyleSheet()\n",
    "        body = styles[\"BodyText\"]\n",
    "\n",
    "        story = []\n",
    "        # Treat double newlines as paragraph breaks; keep single newlines as <br/>\n",
    "        for para in (text or \"\").split(\"\\n\\n\"):\n",
    "            safe = escape(para).replace(\"\\n\", \"<br/>\")\n",
    "            story.append(Paragraph(safe if safe.strip() else \"&nbsp;\", body))\n",
    "            story.append(Spacer(1, 8))\n",
    "\n",
    "        doc.build(story)\n",
    "        buf.seek(0)\n",
    "        return buf.getvalue()\n",
    "    \n",
    "    def _extract_cited_idxs(self, answer: str) -> list[int]:\n",
    "        # Matches [#1], [#12], etc. (also tolerates stray [1])\n",
    "        nums = set(int(n) for n in re.findall(r\"\\[#?(\\d+)\\]\", answer))\n",
    "        return sorted(nums)\n",
    "\n",
    "    def _rag_answer(self, rag_nl, question, k: int = 5, temperature: float = 0.2):\n",
    "\n",
    "        # question = f'CREATE A SECTION OF COMPANY PROFILE USING LAST YEARS OF ANNUAL REPORT PRESENT IN THE CONTEXT FOR {self.company_name}. IF ANY INFORMATION IS NOT FOUND STATE AS n.a. .\\n\\n THIS IS THE SECTION TO BE BUILT: \\n {section7}  \\n USE THIS TO GUIDE YOURSELF ON SEMANTIC TERMS AND HOW TO CALCULATE: \\n {finance_calculations}'\n",
    "        \n",
    "        mode, hits = self._retrieve_hybrid_enhanced(\n",
    "            # query=rag_q, \n",
    "            query_nl=rag_nl,\n",
    "            k=25\n",
    "            )\n",
    "        ctx_text, ctx_items = self._build_context(hits)\n",
    "\n",
    "        system_msg = self.profile_prompt + (\n",
    "            \"\\nWhen you use a fact from the context, add citations like [#1], [#2].\"\n",
    "            \"\\nOnly rely on the numbered context; if a value is missing, say 'n.a.'.\"\n",
    "            f\"\\nIF ANY INFORMATION IS NOT FOUND STATE AS n.a. .\\n\\n USE THIS TO GUIDE YOURSELF ON SEMANTIC TERMS AND HOW TO CALCULATE: \\n {finance_calculations}\"\n",
    "        )\n",
    "        user_msg = f\"Question:\\n{question}\\n\\nContext snippets (numbered):\\n{ctx_text}\"\n",
    "\n",
    "        client = self.az_openai\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\",   \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "        # Try streaming first (SSE). Some networks/proxies block streaming; if so, fall back.\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=AOAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            reasoning_effort=\"high\"\n",
    "        )\n",
    "        answer = resp.choices[0].message.content\n",
    "        mode_model = \"non-streaming (fallback)\"\n",
    "\n",
    "        cited = self._extract_cited_idxs(answer)\n",
    "        used_chunks = [c for c in ctx_items if c[\"i\"] in cited]\n",
    "\n",
    "        # return self._generate_pdf(answer)\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"citations\": cited,          # [1, 3, 7]\n",
    "            \"used_chunks\": used_chunks,  # detailed dicts for each cited snippet\n",
    "            \"all_chunks\": ctx_items,     # everything you sent (optional)\n",
    "            \"mode\": mode                 # retrieval mode info (optional)\n",
    "        }\n",
    "\n",
    "    def _web_search(self, messages):\n",
    "        resp = self.web_openai.responses.create(\n",
    "            model='gpt-5',\n",
    "            input=messages,\n",
    "            tools=[{\"type\": \"web_search\"}],\n",
    "            tool_choice=\"auto\",\n",
    "            # max_output_tokens=self.max_output_tokens,\n",
    "            reasoning={\"effort\": self.reasoning_effort},\n",
    "            text={\"verbosity\": self.verbosity},\n",
    "        )\n",
    "        \n",
    "        return resp.output_text\n",
    "    \n",
    "    def _answer(self, question, ctx_text, k: int = 5, temperature: float = 0.2):\n",
    "\n",
    "        system_msg = self.profile_prompt + (\n",
    "            \"\\nWhen you use a fact from the context, add citations like [#1], [#2].\"\n",
    "            \"\\nOnly rely on the numbered context; if a value is missing, say 'n.a.'.\"\n",
    "        )\n",
    "        user_msg = f\"Question:\\n{question}\\n\\nContext snippets (numbered):\\n{ctx_text}\"\n",
    "\n",
    "        client = self.az_openai\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_msg},\n",
    "            {\"role\": \"user\",   \"content\": user_msg},\n",
    "        ]\n",
    "\n",
    "        # Try streaming first (SSE). Some networks/proxies block streaming; if so, fall back.\n",
    "        \n",
    "        resp = client.chat.completions.create(\n",
    "            model=AOAI_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            reasoning_effort=\"high\"\n",
    "        )\n",
    "        answer = resp.choices[0].message.content\n",
    "\n",
    "        cited = self._extract_cited_idxs(answer)\n",
    "\n",
    "        # return self._generate_pdf(answer)\n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"citations\": cited,          # [1, 3, 7]\n",
    "        }   \n",
    "    \n",
    "    @staticmethod\n",
    "    def has_na(text: str) -> bool:\n",
    "        # match \"n.a.\" or \"n/a\" (case-insensitive)\n",
    "        return bool(re.search(r\"\\b(n\\.a\\.|n/a)\\b\", text, flags=re.I))\n",
    "\n",
    "    def _sections(self, pairs):\n",
    "\n",
    "        answers = []\n",
    "\n",
    "        max_extra_na_retries = 1        # try again at most 2 times (total <= 3 calls per item)\n",
    "        base_delay_seconds = 3.0        # polite delay between attempts\n",
    "\n",
    "\n",
    "        for q, r in pairs:\n",
    "            tries = 0\n",
    "            while True:\n",
    "                if tries > 0:\n",
    "                    # small incremental delay before re-trying\n",
    "                    time.sleep(base_delay_seconds + 0.5 * tries)\n",
    "\n",
    "                resp = self._rag_answer(rag_nl=r[0], question=q[0])\n",
    "                answer_text = resp[\"answer\"]\n",
    "\n",
    "                # stop if good answer OR we've exhausted retries\n",
    "                if not profileAgent.has_na(answer_text) or tries >= max_extra_na_retries:\n",
    "                    answers.append(answer_text)\n",
    "                    break\n",
    "\n",
    "                # otherwise, try again\n",
    "                tries += 1\n",
    "\n",
    "            # optional small gap between different (r,q) items\n",
    "            time.sleep(5.0)\n",
    "        \n",
    "        return answers\n",
    "    \n",
    "    def _generate_section(self, section):\n",
    "\n",
    "        if section == 'GENERATE BUSINESS OVERVIEW':\n",
    "            # =========== GENERATE BUSINESS OVERVIEW\n",
    "            biz_overview_pairs_flat = list(zip(biz_overview_pairs[1], biz_overview_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs = biz_overview_pairs_flat)\n",
    "            resp = self._answer(question=business_overview_formatting, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE KEY STAKEHOLDERS':\n",
    "        # =========== GENERATE KEY STAKEHOLDERS\n",
    "            stakeholders_pairs_flat = list(zip(stakeholders_pairs[1], stakeholders_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs= stakeholders_pairs_flat)\n",
    "            resp = self._answer(question=stakeholders_formatting_2, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE FINANCIAL HIGHLIGHTS':\n",
    "            # =========== GENERATE FINANCIAL HIGHLIGHTS\n",
    "            finance_pairs_flat = list(zip(finance_pairs[1], finance_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs=finance_pairs_flat)\n",
    "            resp = self._answer(question=finance_formatting_2, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE CAPITAL STRUCTURE':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            capital_pairs_flat = list(zip(capital_pairs[1], capital_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs= capital_pairs_flat)\n",
    "            resp = self._answer(question=capital_structure_formatting_2, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE REVENUE SPLIT':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            revenue_pairs_flat = list(zip(revenue_pairs[1], revenue_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "            section_built = self._sections(pairs= revenue_pairs_flat)\n",
    "            resp = self._answer(question=section3, ctx_text=section_built)\n",
    "            return resp['answer']\n",
    "        elif section == 'GENERATE PRODUCTS SERVICES OVERVIEW':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{section4a}'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp = self._web_search(messages)\n",
    "            return resp \n",
    "        elif section == 'GENERATE GEO FOOTPRINT':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{section4b}'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp = self._web_search(messages)\n",
    "            return resp\n",
    "        elif section == 'GENERATE DEVELOPMENTS HIGHLIGHTS':\n",
    "            # =========== GENERATE CAPITAL STRUCTURE\n",
    "            new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{section5}'\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "                {\"role\": \"user\",   \"content\": new_section},\n",
    "            ]\n",
    "            resp = self._web_search(messages)\n",
    "            return resp\n",
    "\n",
    "\n",
    "    def generate_company_profile(self):\n",
    "\n",
    "        # =========== GENERATE BUSINESS OVERVIEW\n",
    "        biz_overview_pairs_flat = list(zip(biz_overview_pairs[1], biz_overview_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "        section1 = self._sections(pairs = biz_overview_pairs_flat)\n",
    "        resp = self._answer(question=business_overview_formatting, ctx_text=section1)\n",
    "        doc = insert_biz_overview(resp['answer'])\n",
    "\n",
    "        time.sleep(60)\n",
    "        # # =========== GENERATE KEY STAKEHOLDERS\n",
    "        # stakeholders_pairs_flat = list(zip(stakeholders_pairs[1], stakeholders_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "        # section2 = self._sections(pairs= stakeholders_pairs_flat)\n",
    "        # resp = self._answer(question=stakeholders_formatting, ctx_text=section2)\n",
    "        # doc = insert_stakeholders(resp['answer'], doc=doc)\n",
    "        \n",
    "        # # time.sleep(60)\n",
    "        # # =========== GENERATE FINANCIAL HIGHLIGHTS\n",
    "        # finance_pairs_flat = list(zip(finance_pairs[1], finance_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "        # section3 = self._sections(pairs=finance_pairs_flat)\n",
    "        # resp = self._answer(question=finance_formatting, ctx_text=section3)\n",
    "        # doc = insert_finance(resp['answer'], doc=doc)\n",
    "\n",
    "        # time.sleep(60)\n",
    "        # # =========== GENERATE CAPITAL STRUCTURE\n",
    "        # capital_pairs_flat = list(zip(capital_pairs[1], capital_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "        # section4 = self._sections(pairs= capital_pairs_flat)\n",
    "        # resp = self._answer(question=capital_structure_formatting, ctx_text=section4)\n",
    "        # doc = insert_capital_structure(resp['answer'], doc=doc)\n",
    "        # time.sleep(60)\n",
    "        # ============ 'GENERATE REVENUE SPLIT'\n",
    "        revenue_pairs_flat = list(zip(revenue_pairs[1], revenue_pairs[0]))  # [(r, q), (r, q), ...]\n",
    "        section_built = self._sections(pairs= revenue_pairs_flat)\n",
    "        resp = self._answer(question=section3, ctx_text=section_built)\n",
    "        doc = insert_revenue_split(resp['answer'], doc=doc) \n",
    "        time.sleep(60)\n",
    "        # =========== 'GENERATE PRODUCTS SERVICES OVERVIEW':\n",
    "        # new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{section4a}'\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "        #     {\"role\": \"user\",   \"content\": new_section},\n",
    "        # ]\n",
    "        # resp = self._web_search(messages)\n",
    "        # doc = insert_services_overview(resp, doc=doc)\n",
    "        # time.sleep(60)\n",
    "        # # ========= 'GENERATE GEO FOOTPRINT':\n",
    "        # new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{section4b}'\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "        #     {\"role\": \"user\",   \"content\": new_section},\n",
    "        # ]\n",
    "        # resp = self._web_search(messages)\n",
    "        # doc = insert_geo_footprint(resp, doc=doc)\n",
    "        # time.sleep(60)\n",
    "        # # ======== 'GENERATE DEVELOPMENTS HIGHLIGHTS':\n",
    "        # new_section = f'All instructions applies to the company: {self.company_name}\\n\\n{section5}'\n",
    "        # messages = [\n",
    "        #     {\"role\": \"system\", \"content\": default_gpt_prompt},\n",
    "        #     {\"role\": \"user\",   \"content\": new_section},\n",
    "        # ]\n",
    "        # resp = self._web_search(messages)\n",
    "        # doc = insert_development_highlights(resp, doc=doc)\n",
    "        # time.sleep(60)\n",
    "    \n",
    "        pdf_bytes = docx_bytes_to_pdf_bytes(doc)\n",
    "\n",
    "        return pdf_bytes, doc\n",
    "        # =========== UNION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b7f8353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JAMES_DONALDSON_GROUP_LTD': 'JAMES DONALDSON GROUP LTD',\n",
       " 'RADLEY_+_CO._LIMITED': 'RADLEY + CO. LIMITED',\n",
       " 'SEAPORT_TOPCO_LIMITED': 'SEAPORT TOPCO LIMITED',\n",
       " 'ASCOT_LLOYD_LIMITED': 'ASCOT LLOYD LIMITED',\n",
       " 'VITA_(HOLDINGS)_LIMITED': 'VITA (HOLDINGS) LIMITED'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from pages.design.dialogues import *\n",
    "from prompts4 import section7, finance_calculations, system_mod\n",
    "from azure.blob_functions import get_companies\n",
    "\n",
    "name_map, names = get_companies()\n",
    "\n",
    "name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37cbebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = profileAgent(\n",
    "            company_name = 'JAMES_DONALDSON_GROUP_LTD',\n",
    "            k=50, \n",
    "            max_text_recall_size=35, \n",
    "            max_chars=10000,\n",
    "            model='gpt-5', \n",
    "            profile_prompt= system_mod, \n",
    "            finance_calculations= finance_calculations\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "075d7a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n",
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document written\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "k is not a known attribute of class <class 'azure.search.documents._generated.models._models_py3.VectorizableTextQuery'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated document written\n",
      "[docx2pdf] conversion failed: TypeError(\"memoryview: a bytes-like object is required, not 'Document'\")\n"
     ]
    }
   ],
   "source": [
    "pdf, doc = agent.generate_company_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78c86f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.save(\"report.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5a787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddd462b",
   "metadata": {},
   "source": [
    "# Business Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b99f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_BREAK\n",
    "\n",
    "# =========================\n",
    "# 0) Your full GPT response\n",
    "# =========================\n",
    "gpt_output = r\"\"\"\n",
    "- Seaport Topco Limited’s principal activity continues to include the research and development of pharmaceutical instrumentation. [1][2]\n",
    "- The company is engaged in group-level operations focused on developing pharmaceutical instrumentation through ongoing research and development initiatives. [1][2]\n",
    "- It offers research and development of pharmaceutical instrumentation as its primary product and service offering. [1][2]\n",
    "- The company reported a headcount of 418 at Dec-24, up from 405, indicating ongoing operations across the group. [9]\n",
    "- The company faces near-term liquidity and working capital pressure, with creditors due within one year rising to £64.7m at Dec-24 driven by £7.7m of new loans and a £9.2m net revolving credit facility draw, while cash generated from operating activities declined to £10.4m from £15.3m in FY23. [9]\n",
    "- It carries substantial longer-term obligations, with creditors due after more than one year at £168.7m at Dec-23, alongside rising trade debtors to £17.0m and reduced stock to £13.2m at Dec-24, indicating cash conversion and balance sheet pressures. [7][8][9]\n",
    "\n",
    "Sources:\n",
    "- [1][2] Annual Report (FY23), Strategic/Directors’ Report, p.5 — Principal activity.\n",
    "- [9] Annual Report (FY24), Group Strategic Report (pp.1–5; exact page n.a.) — Review of the business and future developments, working capital and liquidity movements.\n",
    "- [7][8] Annual Report (FY23), Notes 21–22, Consolidated Statement of Financial Position, p.14 — Creditors due within one year and after more than one year.\n",
    "\"\"\".strip(\"\\n\")\n",
    "\n",
    "# =========================\n",
    "# 1) Open DOCX\n",
    "# =========================\n",
    "doc_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile2.docx\"\n",
    "doc = Document(doc_path)\n",
    "\n",
    "PLACEHOLDER = \"[INSERT BUSINESS OVERVIEW]\"\n",
    "\n",
    "def set_paragraph_multiline(paragraph, text: str):\n",
    "    \"\"\"Replace a paragraph's text with multi-line content, preserving line breaks.\"\"\"\n",
    "    # clear existing runs\n",
    "    for run in paragraph.runs:\n",
    "        run.text = \"\"\n",
    "    # write lines with explicit line breaks\n",
    "    lines = (text or \"\").splitlines()\n",
    "    if not lines:\n",
    "        return\n",
    "    paragraph.add_run(lines[0])\n",
    "    for ln in lines[1:]:\n",
    "        r = paragraph.add_run()\n",
    "        r.add_break(WD_BREAK.LINE)\n",
    "        paragraph.add_run(ln)\n",
    "\n",
    "def replace_placeholder(document: Document, placeholder: str, new_text: str) -> bool:\n",
    "    \"\"\"Find placeholder in paragraphs/cells and replace it with new_text (multiline).\"\"\"\n",
    "    # plain paragraphs\n",
    "    for p in document.paragraphs:\n",
    "        if placeholder in p.text:\n",
    "            set_paragraph_multiline(p, new_text)\n",
    "            return True\n",
    "    # inside tables\n",
    "    for tbl in document.tables:\n",
    "        for row in tbl.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    if placeholder in p.text:\n",
    "                        set_paragraph_multiline(p, new_text)\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "# =========================\n",
    "# 2) Replace the placeholder\n",
    "# =========================\n",
    "ok = replace_placeholder(doc, PLACEHOLDER, gpt_output)\n",
    "if not ok:\n",
    "    print(f\"WARNING: placeholder not found: {PLACEHOLDER}\")\n",
    "\n",
    "# =========================\n",
    "# 3) Save\n",
    "# =========================\n",
    "out_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile2.docx\"\n",
    "doc.save(out_path)\n",
    "print(f\"Updated document written to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d310c3f",
   "metadata": {},
   "source": [
    "# Capital Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e886ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_BREAK\n",
    "\n",
    "# =========================\n",
    "# 0) Your full GPT response\n",
    "# =========================\n",
    "gpt_output = r\"\"\"\n",
    "Metric,FY24,FY23,FY22\n",
    "\"Facility Name\",\"n.a.\",\"Facility B1 (EUR term loan); Facility B2 (USD term loan); Revolving Credit Facility; Delayed Drawdown Facility [#2][#3]\",\"Revolving Credit Facility; Delayed Drawdown Facility [#3]\"\n",
    "\"Interest Rate\",\"n.a.\",\"Euribor + 6.25%; Term SOFR + 6.25% [#2][#3]\",\"n.a.\"\n",
    "Maturity,\"n.a.\",\"Aug-29 [#2][#3]\",\"n.a.\"\n",
    "\"Adjusted EBITDA\",\"n.a.\",\"£17.9m [#2][#3]\",\"£10.6m [#2][#3]\"\n",
    "\"Cash (Closing Cash)\",\"n.a.\",\"£11.8m [#2][#3]\",\"n.a.\"\n",
    "\"Net Debt\",\"n.a.\",\"£171.9m [#2][#3]\",\"n.a.\"\n",
    "Liquidity,\"n.a.\",\"£25.8m [#2][#3]\",\"n.a.\"\n",
    "\"Leverage (Net Debt/EBITDA)\",\"n.a.\",\"9.6x [#2][#3]\",\"n.a.\"\n",
    "\"Facility B1 outstanding (GBP)\",\"n.a.\",\"£36.0m [#2][#3]\",\"n.a.\"\n",
    "\"Facility B2 outstanding (GBP)\",\"n.a.\",\"£135.0m [#2][#3]\",\"n.a.\"\n",
    "\"RCF drawn\",\"n.a.\",\"£16.0m [#2][#3]\",\"£16.0m [#3]\"\n",
    "\"RCF facility size\",\"n.a.\",\"£30.0m [#2][#3]\",\"£30.0m [#3]\"\n",
    "\"Delayed Drawdown Facility size\",\"n.a.\",\"£75.0m [#2][#3]\",\"£75.0m [#3]\"\n",
    "\"Bank loans due after >5 years\",\"n.a.\",\"£168.7m [#2][#3]\",\"n.a.\"\n",
    "\"Bank loans due within 1 year\",\"n.a.\",\"£14.7m [#2][#3]\",\"n.a.\"\n",
    "\"Bank loans + RCF outstanding (excl. leases)\",\"n.a.\",\"£187.0m [#2][#3]\",\"n.a.\"\n",
    "\n",
    "Summary / Interpretation\n",
    "- FY23 leverage is high at 9.6x, based on £171.9m net debt and £17.9m Adjusted EBITDA.\n",
    "- FY23 facility mix is dominated by term loans (Facility B1 ~£36.0m and B2 ~£135.0m) maturing in Aug-29, plus a £30.0m RCF (of which £16.0m was drawn) and a £75.0m delayed draw facility.\n",
    "- FY23 liquidity appears modest at £25.8m, combining £11.8m closing cash with remaining headroom on the £30.0m RCF (drawn £16.0m).\n",
    "- Maturity profile in FY23 is back-ended: £168.7m due after >5 years versus £14.7m due within 1 year; both term facilities mature in Aug-29.\n",
    "- Total FY23 bank loans + RCF outstanding (excl. leases) sums to £187.0m, indicating a sizeable secured debt stack.\n",
    "- FY24 disclosures are not available in the provided excerpts, and FY22 data is limited beyond RCF details.\n",
    "\n",
    "Sources\n",
    "- [#2] Seaport Topco Limited Annual Report (file date 25-Sep-24), pp. 8, 45, 52 — p.8 Adjusted EBITDA (£17.9m FY23; £10.6m FY22); p.45 Loans note (Facility B1/B2 amounts, Aug-29 maturities, interest margins; RCF £30.0m and ~£16.0m drawn; £75.0m delayed draw facility; bank loans due >5 years £168.7m and within 1 year £14.7m); p.52 Net debt analysis (Net Debt £171.9m; closing cash £11.8m). Link: https://aiprojectteneo.blob.core.windows.net/companieshouselinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_0.pdf\n",
    "- [#3] Seaport Topco Limited Annual Report (file date 25-Sep-24), pp. 8, 36, 45, 52 — corroborates Adjusted EBITDA figures; Going concern (p.36) notes RCF drawn £16.0m at Dec-22; Loans note (p.45) for facility sizes/draws and maturities; Net debt analysis (p.52) for Net Debt and closing cash. Link: https://aiprojectteneo.blob.core.windows.net/companieshousinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_1.pdf\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# 1) Extract CSV + SUMMARY  (KEEP sources)\n",
    "# =========================\n",
    "parts = gpt_output.split(\"\\n\\nSummary / Interpretation\", 1)\n",
    "csv_block = parts[0].strip()\n",
    "\n",
    "start = csv_block.find(\"Metric,\")\n",
    "if start == -1:\n",
    "    raise ValueError(\"CSV header 'Metric,' not found in model output.\")\n",
    "csv_block = csv_block[start:]\n",
    "\n",
    "summary_text = \"\"\n",
    "if len(parts) > 1:\n",
    "    summary_text = \"Summary / Interpretation\" + parts[1].rstrip()\n",
    "\n",
    "# =========================\n",
    "# 2) Parse CSV to DataFrame\n",
    "# =========================\n",
    "df = pd.read_csv(io.StringIO(csv_block))\n",
    "expected_cols = {\"Metric\",\"FY24\",\"FY23\",\"FY22\"}\n",
    "if not expected_cols.issubset(df.columns):\n",
    "    raise ValueError(f\"Capital Structure CSV columns missing. Got: {list(df.columns)}\")\n",
    "\n",
    "csv_rows = {\n",
    "    str(df.at[i, \"Metric\"]).strip(): {\n",
    "        \"FY24\": str(df.at[i, \"FY24\"]),\n",
    "        \"FY23\": str(df.at[i, \"FY23\"]),\n",
    "        \"FY22\": str(df.at[i, \"FY22\"]),\n",
    "    }\n",
    "    for i in range(len(df))\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 3) Open DOCX, locate the Capital Structure table\n",
    "# =========================\n",
    "doc_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile2.docx\"\n",
    "doc = Document(doc_path)\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "def tokens(s: str) -> set:\n",
    "    return set(re.findall(r\"[a-z0-9]+\", (s or \"\").lower()))\n",
    "\n",
    "def jaccard(a: str, b: str) -> float:\n",
    "    ta, tb = tokens(a), tokens(b)\n",
    "    if not ta or not tb:\n",
    "        return 0.0\n",
    "    inter = len(ta & tb)\n",
    "    union = len(ta | tb)\n",
    "    return inter / union if union else 0.0\n",
    "\n",
    "def find_cap_struct_table(document: Document):\n",
    "    # 1) after 'Capital Structure' heading\n",
    "    found_heading = False\n",
    "    body = document._element.body\n",
    "    for child in body.iterchildren():\n",
    "        tag = child.tag.rsplit(\"}\", 1)[-1]\n",
    "        if tag == \"p\":\n",
    "            p_text = \"\".join(t.text for t in child.iter()\n",
    "                             if t.tag.rsplit(\"}\",1)[-1] == \"t\").strip()\n",
    "            if norm(p_text) == \"capitalstructure\":\n",
    "                found_heading = True\n",
    "        elif tag == \"tbl\" and found_heading:\n",
    "            from docx.table import Table\n",
    "            return Table(child, document)\n",
    "\n",
    "    # 2) heuristic by content\n",
    "    for tbl in document.tables:\n",
    "        row_texts = [\" \".join(c.text for c in r.cells) for r in tbl.rows]\n",
    "        joined = \" \".join(row_texts)\n",
    "        if all(x in norm(joined) for x in [\"ebitda\",\"leverage\"]):\n",
    "            return tbl\n",
    "    return None\n",
    "\n",
    "table = find_cap_struct_table(doc)\n",
    "if table is None:\n",
    "    raise RuntimeError(\"Could not locate the 'Capital Structure' table.\")\n",
    "\n",
    "# Identify FY columns\n",
    "def find_fy_cols(tbl):\n",
    "    for r_i in range(min(2, len(tbl.rows))):\n",
    "        labels = [norm(c.text) for c in tbl.rows[r_i].cells]\n",
    "        loc = {}\n",
    "        for idx, txt in enumerate(labels):\n",
    "            if txt == \"fy24\": loc[\"FY24\"] = idx\n",
    "            if txt == \"fy23\": loc[\"FY23\"] = idx\n",
    "            if txt == \"fy22\": loc[\"FY22\"] = idx\n",
    "        if {\"FY24\",\"FY23\",\"FY22\"}.issubset(loc.keys()):\n",
    "            return loc[\"FY24\"], loc[\"FY23\"], loc[\"FY22\"]\n",
    "    if len(tbl.rows[0].cells) >= 4:\n",
    "        return 1, 2, 3\n",
    "    raise RuntimeError(\"Could not determine FY columns in Capital Structure table.\")\n",
    "\n",
    "col_FY24, col_FY23, col_FY22 = find_fy_cols(table)\n",
    "\n",
    "# Build row index from first column (labels)\n",
    "doc_row_index = {}\n",
    "doc_row_labels = {}  # norm_label -> raw label (for debug)\n",
    "for r_idx, row in enumerate(table.rows):\n",
    "    if not row.cells:\n",
    "        continue\n",
    "    label_raw = row.cells[0].text.strip()\n",
    "    if label_raw:\n",
    "        key = norm(label_raw)\n",
    "        doc_row_index[key] = r_idx\n",
    "        doc_row_labels[key] = label_raw\n",
    "\n",
    "# =========================\n",
    "# 4) Mapping (CSV -> DOC label), with synonyms & typo tolerance\n",
    "# =========================\n",
    "def keynorm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "# Add broad synonyms, incl. likely template wordings\n",
    "metric_to_doc_syns = {\n",
    "    keynorm(\"Facility Name\"): [\n",
    "        \"Facility Name\", \"Name of the Facility\", \"Facility\", \"Facilities\", \"Facility Names\",\n",
    "        \"Name of Facility\"\n",
    "    ],\n",
    "    keynorm(\"Interest Rate\"): [\"Interest Rate\", \"Interst Rate\", \"Rate\", \"Interest\"],\n",
    "    keynorm(\"Interst Rate\"):  [\"Interest Rate\", \"Interst Rate\", \"Rate\", \"Interest\"],\n",
    "    keynorm(\"Maturity\"): [\"Maturity\", \"Final Maturity\", \"Maturities\", \"Maturity Date\"],\n",
    "    keynorm(\"Adjusted EBITDA\"): [\"EBITDA\", \"Adjusted EBITDA\"],\n",
    "    keynorm(\"Cash (Closing Cash)\"): [\n",
    "        \"Cash (Closing Cash)\", \"Cash (Closing cash)\", \"Closing Cash\",\n",
    "        \"Cash\", \"Cash and cash equivalents\", \"Cash & cash equivalents\"\n",
    "    ],\n",
    "    keynorm(\"Net Debt\"): [\"Net External Debt\", \"Net Debt\"],\n",
    "    keynorm(\"Liquidity\"): [\"Liquidity\"],\n",
    "    keynorm(\"Leverage (Net Debt/EBITDA)\"): [\"Leverage\"],\n",
    "    keynorm(\"Leverage (Net Debt / EBITDA)\"): [\"Leverage\"],\n",
    "    keynorm(\"Bank loans + RCF outstanding (excl. leases)\"): [\"Gross External Debt\", \"Total External Debt\"],\n",
    "\n",
    "    keynorm(\"Facility B1 outstanding (GBP)\"): [\"Amount Outstanding\"],\n",
    "    keynorm(\"Facility B2 outstanding (GBP)\"): [\"Amount Outstanding\"],\n",
    "    keynorm(\"RCF drawn\"): [\"Amount Outstanding\"],\n",
    "\n",
    "    keynorm(\"RCF facility size\"): [None],\n",
    "    keynorm(\"Delayed Drawdown Facility size\"): [None],\n",
    "    keynorm(\"Bank loans due after >5 years\"): [None],\n",
    "    keynorm(\"Bank loans due within 1 year\"): [None],\n",
    "}\n",
    "\n",
    "def smart_lookup_row_index(label_candidates):\n",
    "    \"\"\"\n",
    "    Resolve to the best row index by:\n",
    "      1) Exact normalized match\n",
    "      2) Contains match (both directions)\n",
    "      3) Token Jaccard similarity >= 0.5\n",
    "    Returns row_index or None.\n",
    "    \"\"\"\n",
    "    cand_norms = [norm(c) for c in label_candidates if c]\n",
    "\n",
    "    # 1) exact\n",
    "    for cn in cand_norms:\n",
    "        if cn in doc_row_index:\n",
    "            return doc_row_index[cn]\n",
    "\n",
    "    # 2) contains (prefer the longest doc label match)\n",
    "    best_idx = None\n",
    "    best_len = -1\n",
    "    for cn in cand_norms:\n",
    "        for dl_norm, idx in doc_row_index.items():\n",
    "            if cn and (cn in dl_norm or dl_norm in cn):\n",
    "                if len(dl_norm) > best_len:\n",
    "                    best_idx, best_len = idx, len(dl_norm)\n",
    "    if best_idx is not None:\n",
    "        return best_idx\n",
    "\n",
    "    # 3) token overlap\n",
    "    best_idx = None\n",
    "    best_score = 0.0\n",
    "    for cn in cand_norms:\n",
    "        for dl_norm, idx in doc_row_index.items():\n",
    "            score = jaccard(cn, dl_norm)\n",
    "            if score >= 0.5 and score > best_score:\n",
    "                best_idx, best_score = idx, score\n",
    "    return best_idx\n",
    "\n",
    "# =========================\n",
    "# 5) Populate the table (incl. Facility Name / Interest Rate / Maturity)\n",
    "#    and aggregate facility amounts into 'Amount Outstanding'\n",
    "# =========================\n",
    "agg_amount = {\"FY24\": [], \"FY23\": [], \"FY22\": []}\n",
    "\n",
    "def maybe_append(prefix, v):\n",
    "    v = (v or \"\").strip()\n",
    "    if not v or v.lower() == \"n.a.\":\n",
    "        return None\n",
    "    return f\"{prefix}: {v}\"\n",
    "\n",
    "unmapped_metrics = []\n",
    "\n",
    "for csv_metric, years in csv_rows.items():\n",
    "    mkey = keynorm(csv_metric)\n",
    "    syns = metric_to_doc_syns.get(mkey, [csv_metric])\n",
    "\n",
    "    # aggregated targets\n",
    "    if any((s and norm(s) == norm(\"Amount Outstanding\")) for s in syns if s):\n",
    "        if mkey == keynorm(\"Facility B1 outstanding (GBP)\"):\n",
    "            for fy in (\"FY24\", \"FY23\", \"FY22\"):\n",
    "                s = maybe_append(\"B1\", years[fy]);  agg_amount[fy].append(s) if s else None\n",
    "        elif mkey == keynorm(\"Facility B2 outstanding (GBP)\"):\n",
    "            for fy in (\"FY24\", \"FY23\", \"FY22\"):\n",
    "                s = maybe_append(\"B2\", years[fy]);  agg_amount[fy].append(s) if s else None\n",
    "        elif mkey == keynorm(\"RCF drawn\"):\n",
    "            for fy in (\"FY24\", \"FY23\", \"FY22\"):\n",
    "                s = maybe_append(\"RCF drawn\", years[fy]);  agg_amount[fy].append(s) if s else None\n",
    "        continue\n",
    "\n",
    "    r_idx = smart_lookup_row_index(syns)\n",
    "    if r_idx is None:\n",
    "        unmapped_metrics.append(csv_metric)\n",
    "        continue\n",
    "\n",
    "    row = table.rows[r_idx]\n",
    "    row.cells[col_FY24].text = years[\"FY24\"]\n",
    "    row.cells[col_FY23].text = years[\"FY23\"]\n",
    "    row.cells[col_FY22].text = years[\"FY22\"]\n",
    "\n",
    "# Aggregated 'Amount Outstanding'\n",
    "amount_row_idx = smart_lookup_row_index([\"Amount Outstanding\"])\n",
    "if amount_row_idx is not None:\n",
    "    row = table.rows[amount_row_idx]\n",
    "    row.cells[col_FY24].text = \"; \".join(agg_amount[\"FY24\"]) if agg_amount[\"FY24\"] else \"n.a.\"\n",
    "    row.cells[col_FY23].text = \"; \".join(agg_amount[\"FY23\"]) if agg_amount[\"FY23\"] else \"n.a.\"\n",
    "    row.cells[col_FY22].text = \"; \".join(agg_amount[\"FY22\"]) if agg_amount[\"FY22\"] else \"n.a.\"\n",
    "\n",
    "# =========================\n",
    "# 6) Insert the full SUMMARY (including Sources) — no duplicates\n",
    "# =========================\n",
    "PLACEHOLDER = \"[INSERT CAPITAL STRUCTURE SUMMARY]\"\n",
    "HEADING_TEXT = \"Summary / Interpretation\"\n",
    "\n",
    "def set_paragraph_multiline(paragraph, text: str):\n",
    "    for run in paragraph.runs:  # clear\n",
    "        run.text = \"\"\n",
    "    lines = (text or \"\").splitlines()\n",
    "    if not lines:\n",
    "        return\n",
    "    paragraph.add_run(lines[0])\n",
    "    for ln in lines[1:]:\n",
    "        paragraph.add_run().add_break(WD_BREAK.LINE)\n",
    "        paragraph.add_run(ln)\n",
    "\n",
    "def replace_placeholder(document: Document, placeholder: str, new_text: str) -> bool:\n",
    "    # paragraphs\n",
    "    for p in document.paragraphs:\n",
    "        if placeholder in p.text:\n",
    "            set_paragraph_multiline(p, new_text)\n",
    "            return True\n",
    "    # tables\n",
    "    for tbl in document.tables:\n",
    "        for row in tbl.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    if placeholder in p.text:\n",
    "                        set_paragraph_multiline(p, new_text)\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "def find_all_summary_paragraphs(document: Document, heading_text: str):\n",
    "    anchors = []\n",
    "    for p in document.paragraphs:\n",
    "        if heading_text in p.text:\n",
    "            anchors.append(p)\n",
    "    for tbl in document.tables:\n",
    "        for row in tbl.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    if heading_text in p.text:\n",
    "                        anchors.append(p)\n",
    "    return anchors\n",
    "\n",
    "insert_done = False\n",
    "if summary_text:\n",
    "    insert_done = replace_placeholder(doc, PLACEHOLDER, summary_text)\n",
    "\n",
    "if summary_text and not insert_done:\n",
    "    anchors = find_all_summary_paragraphs(doc, HEADING_TEXT)\n",
    "    if anchors:\n",
    "        # overwrite first and remove extras\n",
    "        set_paragraph_multiline(anchors[0], summary_text)\n",
    "        for dup in anchors[1:]:\n",
    "            dup._element.getparent().remove(dup._element)\n",
    "    else:\n",
    "        p = doc.add_paragraph()\n",
    "        set_paragraph_multiline(p, summary_text)\n",
    "        # ensure no accidental multiples\n",
    "        anchors = find_all_summary_paragraphs(doc, HEADING_TEXT)\n",
    "        for dup in anchors[1:]:\n",
    "            dup._element.getparent().remove(dup._element)\n",
    "\n",
    "# =========================\n",
    "# 7) Save + (optional) debug\n",
    "# =========================\n",
    "out_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile2.docx\"\n",
    "doc.save(out_path)\n",
    "print(f\"Updated document written to: {out_path}\")\n",
    "\n",
    "if unmapped_metrics:\n",
    "    print(\"NOTE — CSV metrics that couldn't be matched to any row (check your template labels):\")\n",
    "    for m in unmapped_metrics:\n",
    "        print(\" -\", m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95894b",
   "metadata": {},
   "source": [
    "# Key Stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_BREAK\n",
    "\n",
    "# =========================\n",
    "# 0) Your full GPT response\n",
    "# =========================\n",
    "gpt_output = r\"\"\"\n",
    "Metric,Shareholders\n",
    "\"Shareholders\",n.a.\n",
    "\"Management\",\"Directors (FY24): M Bauer; R Diggelmann — resigned Dec-24; P Dowdy — resigned Feb-25; J Feldman; R Friel — resigned Mar-25; K Murphy; D Newble — resigned Jul-24; A Thorburn; R Walton — appointed Jul-24\"\n",
    "\"Lenders\",n.a.\n",
    "\"Auditors\",\"Grant Thornton UK LLP (Statutory Auditor); Stephen Wyborn (Senior Statutory Auditor)\"\n",
    "\"Advisors\",\"Facility agent: Kroll Agency Services Limited; Bankers: n.a.; Solicitors: n.a.; Financial advisor: n.a.\"\n",
    "\n",
    "Summary / Interpretation\n",
    "- Shareholder information is n.a., indicating no disclosed immediate or ultimate parent in the provided filings.\n",
    "- Management is represented by the FY24 directors list; specific Chairman/CEO/CFO titles are not provided.\n",
    "- Lenders are n.a., suggesting no disclosed bank facilities/borrowings in the available excerpts.\n",
    "- Auditors are identified (Grant Thornton UK LLP; Senior Statutory Auditor Stephen Wyborn), while most other advisors are n.a. except the facility agent (Kroll Agency Services Limited).\n",
    "- Advisor disclosure is limited (bankers/solicitors/financial advisor n.a.), constraining visibility into counterparties.\n",
    "\n",
    "SECTION 3 - SOURCES\n",
    "- [#1] Seaport Topco Limited AA Annual Report (published Sep-25), Directors’ Report for the year ended 31 Dec-24, p.12. Link: https://aiprojectteneo.blob.core.windows.net/companieshousesinglefile/14171962/SEAPORT_TOPCO_LIMITED_AA_annualReport_2025-09-30_0.pdf — used for the “Management” (directors) row.\n",
    "- [#2] Seaport Topco Limited AA Annual Report (2024-09-25) – provided excerpt indicating Company Information page not available — supports n.a. for certain advisor details.\n",
    "- [#3] Seaport Topco Limited annual report (FY24), Notes to the Financial Statements – Accounting policies excerpt (page n.a.) — used to check terminology (bank loans/borrowings); lenders n.a. and facility agent reference noted elsewhere.\n",
    "- [#4] Seaport Topco Limited Annual Report 2024, Independent Auditors’ Report signature block, file: SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_1.pdf (26 pages), signed Apr-24 — used for “Auditors” identification and to support shareholder n.a.\n",
    "- [#5] Seaport Topco Limited Annual Report (published Sep-24), Notes to the Financial Statements, Note 16: Fixed asset investments – Indirect subsidiary undertakings (page n.a.) — used to support “Shareholders” n.a. (no parent/ultimate controlling party disclosed).\n",
    "- [#7] Seaport Topco Limited Annual Report 2024, Independent Auditors’ Report signature block, file: SEAPORT_TOPCO_LIMITED_AA_annualReport_2024-09-25_0.pdf (26 pages), signed Apr-24 — corroborates “Auditors” identification.\n",
    "\"\"\"\n",
    "\n",
    "# =========================\n",
    "# 1) Extract CSV + SUMMARY\n",
    "# =========================\n",
    "parts = gpt_output.split(\"\\n\\nSummary / Interpretation\", 1)\n",
    "csv_block = parts[0].strip()\n",
    "\n",
    "start = csv_block.find(\"Metric,\")\n",
    "if start == -1:\n",
    "    raise ValueError(\"CSV header 'Metric,' not found in model output.\")\n",
    "csv_block = csv_block[start:]\n",
    "\n",
    "summary_text = \"\"\n",
    "if len(parts) > 1:\n",
    "    summary_text = \"Summary / Interpretation\" + parts[1].rstrip()\n",
    "\n",
    "# =========================\n",
    "# 2) Parse CSV to DataFrame\n",
    "# =========================\n",
    "df = pd.read_csv(io.StringIO(csv_block))\n",
    "expected_cols = {\"Metric\", \"Shareholders\"}\n",
    "if not expected_cols.issubset(df.columns):\n",
    "    raise ValueError(f\"Key Stakeholders CSV columns missing. Got: {list(df.columns)}\")\n",
    "\n",
    "# Dict: metric -> value (right column)\n",
    "ks_rows = {\n",
    "    str(df.at[i, \"Metric\"]).strip(): str(df.at[i, \"Shareholders\"]).strip()\n",
    "    for i in range(len(df))\n",
    "}\n",
    "\n",
    "# ==============================================\n",
    "# 3) Open DOCX, find the \"Key Stakeholders\" table\n",
    "# ==============================================\n",
    "doc_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile (1).docx\"\n",
    "doc = Document(doc_path)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "def find_ks_table(document: Document):\n",
    "    # Prefer a table whose header has both \"Title\" and \"Occupants\"\n",
    "    for tbl in document.tables:\n",
    "        if not tbl.rows:\n",
    "            continue\n",
    "        header = [norm(c.text) for c in tbl.rows[0].cells]\n",
    "        if \"title\" in header and \"occupants\" in header:\n",
    "            return tbl\n",
    "\n",
    "    # Fallback: first table after the \"Key Stakeholders\" heading\n",
    "    found_heading = False\n",
    "    body = document._element.body\n",
    "    for child in body.iterchildren():\n",
    "        tag = child.tag.rsplit(\"}\", 1)[-1]\n",
    "        if tag == \"p\":\n",
    "            p_text = \"\".join(t.text for t in child.iter()\n",
    "                             if t.tag.rsplit(\"}\",1)[-1] == \"t\").strip()\n",
    "            if norm(p_text) == \"keystakeholders\":\n",
    "                found_heading = True\n",
    "        elif tag == \"tbl\" and found_heading:\n",
    "            from docx.table import Table\n",
    "            return Table(child, document)\n",
    "    return None\n",
    "\n",
    "ks_table = find_ks_table(doc)\n",
    "if ks_table is None:\n",
    "    raise RuntimeError(\"Could not locate the 'Key Stakeholders' table (Title | Occupants).\")\n",
    "\n",
    "# ==============================================\n",
    "# 4) Detect columns: label = \"Title\", value = \"Occupants\"\n",
    "# ==============================================\n",
    "def get_title_and_occupants_cols(tbl):\n",
    "    # Defaults for a 2-col layout\n",
    "    label_col, value_col = 0, 1\n",
    "\n",
    "    if tbl.rows:\n",
    "        header_norm = [norm(c.text) for c in tbl.rows[0].cells]\n",
    "        if \"title\" in header_norm:\n",
    "            label_col = header_norm.index(\"title\")\n",
    "        if \"occupants\" in header_norm:\n",
    "            value_col = header_norm.index(\"occupants\")\n",
    "\n",
    "    # Ensure they are different; if not, force value_col to the other col\n",
    "    if value_col == label_col and len(tbl.rows[0].cells) >= 2:\n",
    "        value_col = 1 if label_col == 0 else 0\n",
    "    return label_col, value_col\n",
    "\n",
    "label_col, value_col = get_title_and_occupants_cols(ks_table)\n",
    "\n",
    "# ==============================================\n",
    "# 5) Build row index using the Title (label) column\n",
    "# ==============================================\n",
    "row_index = {}\n",
    "for r_idx, row in enumerate(ks_table.rows):\n",
    "    if not row.cells:\n",
    "        continue\n",
    "    # Skip header\n",
    "    if r_idx == 0:\n",
    "        continue\n",
    "    label_text = row.cells[label_col].text.strip()\n",
    "    if label_text:\n",
    "        row_index[norm(label_text)] = r_idx\n",
    "\n",
    "# ==============================================\n",
    "# 6) Map CSV metric names → Title labels\n",
    "# ==============================================\n",
    "def keynorm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "metric_to_title = {\n",
    "    keynorm(\"Shareholders\"): \"Shareholders\",\n",
    "    keynorm(\"Management\"):  \"Management\",\n",
    "    keynorm(\"Lenders\"):     \"Lenders\",\n",
    "    keynorm(\"Auditors\"):    \"Auditors\",\n",
    "    keynorm(\"Advisors\"):    \"Advisors\",\n",
    "}\n",
    "\n",
    "# ==============================================\n",
    "# 7) Populate the Occupants column ONLY\n",
    "# ==============================================\n",
    "not_found = []\n",
    "for metric, value in ks_rows.items():\n",
    "    title_label = metric_to_title.get(keynorm(metric), metric)\n",
    "    r_idx = row_index.get(norm(title_label))\n",
    "    if r_idx is None:\n",
    "        not_found.append(metric)\n",
    "        continue\n",
    "    # write into Occupants cell\n",
    "    ks_table.rows[r_idx].cells[value_col].text = value\n",
    "\n",
    "if not_found:\n",
    "    print(\"WARNING — missing rows for:\", \", \".join(not_found))\n",
    "\n",
    "# ==============================================\n",
    "# 8) Replace the placeholder with the KS SUMMARY text (optional)\n",
    "#     (placeholder: [INSERT KEY STAKEHOLDERS SUMMARY])\n",
    "# ==============================================\n",
    "KS_PLACEHOLDER = \"[INSERT KEY STAKEHOLDERS SUMMARY]\"\n",
    "\n",
    "def set_paragraph_multiline(paragraph, text: str):\n",
    "    for run in paragraph.runs:\n",
    "        run.text = \"\"\n",
    "    lines = (text or \"\").splitlines()\n",
    "    if not lines:\n",
    "        return\n",
    "    paragraph.add_run(lines[0])\n",
    "    for ln in lines[1:]:\n",
    "        paragraph.add_run().add_break(WD_BREAK.LINE)\n",
    "        paragraph.add_run(ln)\n",
    "\n",
    "def replace_placeholder(document: Document, placeholder: str, new_text: str) -> bool:\n",
    "    # paragraphs\n",
    "    for p in document.paragraphs:\n",
    "        if placeholder in p.text:\n",
    "            set_paragraph_multiline(p, new_text)\n",
    "            return True\n",
    "    # cells\n",
    "    for tbl in document.tables:\n",
    "        for row in tbl.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    if placeholder in p.text:\n",
    "                        set_paragraph_multiline(p, new_text)\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "if summary_text:\n",
    "    ok = replace_placeholder(doc, KS_PLACEHOLDER, summary_text)\n",
    "    if not ok:\n",
    "        print(\"NOTE: placeholder not found:\", KS_PLACEHOLDER)\n",
    "\n",
    "# ==============================================\n",
    "# 9) Save\n",
    "# ==============================================\n",
    "out_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile2.docx\"\n",
    "doc.save(out_path)\n",
    "print(f\"Updated document written to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b1fd1d",
   "metadata": {},
   "source": [
    "# Financial Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, re\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.enum.text import WD_BREAK\n",
    "\n",
    "# =========================\n",
    "# 0) Your full GPT response\n",
    "# =========================\n",
    "gpt_output = finance_output\n",
    "\n",
    "# =========================\n",
    "# 1) Extract CSV + SUMMARY\n",
    "# =========================\n",
    "# Split off the summary block (everything after the blank line + heading)\n",
    "parts = gpt_output.split(\"\\n\\nSummary / Interpretation\", 1)\n",
    "csv_block = parts[0].strip()\n",
    "\n",
    "# Ensure we start at the CSV header\n",
    "start = csv_block.find(\"Metric,\")\n",
    "if start == -1:\n",
    "    raise ValueError(\"CSV header 'Metric,' not found in model output.\")\n",
    "csv_block = csv_block[start:]\n",
    "\n",
    "# Summary text (keep heading + bullets if present)\n",
    "summary_text = \"\"\n",
    "if len(parts) > 1:\n",
    "    summary_text = \"Summary / Interpretation\" + parts[1].rstrip()\n",
    "\n",
    "# =========================\n",
    "# 2) Parse CSV to DataFrame\n",
    "# =========================\n",
    "df = pd.read_csv(io.StringIO(csv_block))\n",
    "expected_cols = {\"Metric\",\"FY24\",\"FY23\",\"FY22\"}\n",
    "if not expected_cols.issubset(df.columns):\n",
    "    raise ValueError(f\"CSV columns missing. Got: {list(df.columns)}\")\n",
    "\n",
    "# Dict: metric -> {FY24, FY23, FY22}\n",
    "csv_rows = {\n",
    "    str(df.at[i, \"Metric\"]).strip(): {\n",
    "        \"FY24\": df.at[i, \"FY24\"],\n",
    "        \"FY23\": df.at[i, \"FY23\"],\n",
    "        \"FY22\": df.at[i, \"FY22\"],\n",
    "    }\n",
    "    for i in range(len(df))\n",
    "}\n",
    "\n",
    "# ==============================================\n",
    "# 3) Open DOCX, find the Financial Performance table\n",
    "# ==============================================\n",
    "doc_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile (1).docx\"\n",
    "doc = Document(doc_path)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "def find_fin_perf_table(document: Document):\n",
    "    # Heuristic 1: find a table whose header row contains FY24/FY23/FY22\n",
    "    for tbl in document.tables:\n",
    "        if len(tbl.rows):\n",
    "            header = \" \".join(c.text for c in tbl.rows[0].cells)\n",
    "            if all(x in norm(header) for x in [\"fy24\",\"fy23\",\"fy22\"]):\n",
    "                return tbl\n",
    "    # Heuristic 2: first table after a paragraph exactly \"Financial Performance\"\n",
    "    found_heading = False\n",
    "    body = document._element.body\n",
    "    for child in body.iterchildren():\n",
    "        tag = child.tag.rsplit(\"}\", 1)[-1]\n",
    "        if tag == \"p\":\n",
    "            p_text = \"\".join(t.text for t in child.iter() if t.tag.rsplit(\"}\",1)[-1] == \"t\").strip()\n",
    "            if norm(p_text) == \"financialperformance\":\n",
    "                found_heading = True\n",
    "        elif tag == \"tbl\" and found_heading:\n",
    "            from docx.table import Table\n",
    "            return Table(child, document)\n",
    "    return None\n",
    "\n",
    "table = find_fin_perf_table(doc)\n",
    "if table is None:\n",
    "    raise RuntimeError(\"Could not locate the 'Financial Performance' table.\")\n",
    "\n",
    "# ==============================================\n",
    "# 4) Map CSV metric names → rows in the template\n",
    "# ==============================================\n",
    "# --- Normalizers (use same rule for CSV and map keys) ---\n",
    "def keynorm(s: str) -> str:\n",
    "    return re.sub(r\"[^a-z0-9]+\", \"\", (s or \"\").lower())\n",
    "\n",
    "# Map CSV metric -> DOC row label (left column text in your template)\n",
    "# Use normalized keys on the left so variations in spaces/slashes/plus signs won't break it.\n",
    "metric_map_norm = {\n",
    "    keynorm(\"Revenue (Turnover)\"): \"Revenue\",\n",
    "    keynorm(\"Revenue growth % (yoy)\"): \"Revenue Growth\",\n",
    "    keynorm(\"Gross profit\"): \"Gross Profit\",\n",
    "    keynorm(\"Gross margin %\"): \"Gross Margin\",\n",
    "    keynorm(\"EBITDA\"): \"EBITDA\",\n",
    "    keynorm(\"EBITDA margin %\"): \"EBITDA Margin\",\n",
    "    keynorm(\"Adjusted EBITDA\"): \"Adjusted EBITDA\",\n",
    "\n",
    "    # >>> The ones you said aren't populating <<<\n",
    "    keynorm(\"Capex (tangible+intangible)\"): \"CAPEX\",  # DOC label\n",
    "    keynorm(\"Capex (tangible + intangible)\"): \"CAPEX\",  # alt form (spaces)\n",
    "    keynorm(\"Net Working Capital (change)\"): \"NET_WORK\",\n",
    "    keynorm(\"Cash Flow from Financing Activities (net)\"): \"Cash Flow from Financing Activities\",\n",
    "    keynorm(\"Net cash from financing activities\"): \"CASH_FINAN\",  # alt wording\n",
    "    keynorm(\"Total Debt (external)\"): \"TOTAL_DEBT\",\n",
    "    keynorm(\"Total debt (bank + lease liabilities)\"): \"TOTAL_DEBT\",  # alt wording\n",
    "    keynorm(\"Leverage (Net Debt/EBITDA)\"): \"LEVERAGE\",\n",
    "    keynorm(\"Leverage (Net Debt / EBITDA)\"): \"LEVERAGE\",\n",
    "\n",
    "    # Other cash flow items you have\n",
    "    keynorm(\"Net cash from operating activities\"): \"Cash Flow from Operating Activities\",\n",
    "    keynorm(\"Net Working Capital (change)\"): \"Net Working Capital\",  # duplicate on purpose (case variant)\n",
    "    keynorm(\"Operating cash flow excl. NWC\"): \"Cash Flow from Operating Activities excl. Net Working Capital\",\n",
    "    keynorm(\"Other Cash Flow from Investing Activities\"): \"Other Cash Flow from Investing Activities\",\n",
    "    keynorm(\"Net cash from investing activities\"): \"Net Cash Flow from Investing Activities\",\n",
    "    keynorm(\"CFADS\"): \"CFADS\",\n",
    "    keynorm(\"Opening Cash\"): \"Opening Cash\",\n",
    "    keynorm(\"Change in Cash\"): \"Change in Cash\",\n",
    "    keynorm(\"Closing Cash\"): \"Closing Cash\",\n",
    "    keynorm(\"Bank loans outstanding\"): \"Total Debt\",  # if your template has separate row, adjust\n",
    "    keynorm(\"Net Debt\"): \"Net Debt\",\n",
    "}\n",
    "\n",
    "# Build lookup of row labels in the DOCX (first column). You already did:\n",
    "doc_row_index = {}\n",
    "for r_idx, row in enumerate(table.rows):\n",
    "    label = row.cells[0].text.strip()\n",
    "    if label:\n",
    "        doc_row_index[norm(label)] = r_idx  # norm = your doc normalizer (same idea as keynorm)\n",
    "\n",
    "# Identify FY columns in header row (handles \"FY 24\" vs \"FY24\")\n",
    "header_norm = [norm(c.text) for c in table.rows[0].cells]\n",
    "try:\n",
    "    col_FY24 = header_norm.index(\"fy24\")\n",
    "    col_FY23 = header_norm.index(\"fy23\")\n",
    "    col_FY22 = header_norm.index(\"fy22\")\n",
    "except ValueError:\n",
    "    # If header is the second row in your template, try that\n",
    "    header_norm = [norm(c.text) for c in table.rows[1].cells]\n",
    "    col_FY24 = header_norm.index(\"fy24\")\n",
    "    col_FY23 = header_norm.index(\"fy23\")\n",
    "    col_FY22 = header_norm.index(\"fy22\")\n",
    "\n",
    "\n",
    "# Populate table\n",
    "not_found = []\n",
    "\n",
    "for csv_metric, years in csv_rows.items():\n",
    "    # Find the DOC row label using normalized CSV metric text\n",
    "    target_label = metric_map_norm.get(keynorm(csv_metric), csv_metric)  # fall back to same text\n",
    "    r_idx = doc_row_index.get(norm(target_label))  # norm() is your existing doc normalizer\n",
    "    if r_idx is None:\n",
    "        not_found.append(csv_metric)\n",
    "        continue\n",
    "\n",
    "    row = table.rows[r_idx]\n",
    "    row.cells[col_FY24].text = str(years[\"FY24\"])\n",
    "    row.cells[col_FY23].text = str(years[\"FY23\"])\n",
    "    row.cells[col_FY22].text = str(years[\"FY22\"])\n",
    "\n",
    "# ==============================================\n",
    "# 5) Populate the table from CSV\n",
    "# ==============================================\n",
    "not_found = []\n",
    "\n",
    "for csv_metric, years in csv_rows.items():\n",
    "    target_label = metric_map_norm.get(csv_metric, csv_metric)\n",
    "    r_idx = doc_row_index.get(norm(target_label))\n",
    "    if r_idx is None:\n",
    "        not_found.append(csv_metric)\n",
    "        continue\n",
    "\n",
    "    row = table.rows[r_idx]\n",
    "    row.cells[col_FY24].text = str(years[\"FY24\"])\n",
    "    row.cells[col_FY23].text = str(years[\"FY23\"])\n",
    "    row.cells[col_FY22].text = str(years[\"FY22\"])\n",
    "\n",
    "# ==============================================\n",
    "# 6) Replace the placeholder with the SUMMARY text\n",
    "#     (placeholder: [INSERT FINANCIAL PERFORMANCE SUMMARY])\n",
    "# ==============================================\n",
    "PLACEHOLDER = \"[INSERT FINANCIAL PERFORMANCE SUMMARY]\"\n",
    "\n",
    "def set_paragraph_multiline(paragraph, text: str):\n",
    "    # clear runs\n",
    "    for run in paragraph.runs:\n",
    "        run.text = \"\"\n",
    "    # add lines with explicit line breaks\n",
    "    lines = text.splitlines()\n",
    "    if not lines:\n",
    "        return\n",
    "    paragraph.add_run(lines[0])\n",
    "    for ln in lines[1:]:\n",
    "        paragraph.add_run().add_break(WD_BREAK.LINE)\n",
    "        paragraph.add_run(ln)\n",
    "\n",
    "def replace_placeholder(document: Document, placeholder: str, new_text: str) -> bool:\n",
    "    # search in paragraphs\n",
    "    for p in document.paragraphs:\n",
    "        if placeholder in p.text:\n",
    "            set_paragraph_multiline(p, new_text)\n",
    "            return True\n",
    "    # search inside tables (cells contain their own paragraphs)\n",
    "    for tbl in document.tables:\n",
    "        for row in tbl.rows:\n",
    "            for cell in row.cells:\n",
    "                for p in cell.paragraphs:\n",
    "                    if placeholder in p.text:\n",
    "                        set_paragraph_multiline(p, new_text)\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "if summary_text:\n",
    "    ok = replace_placeholder(doc, PLACEHOLDER, summary_text)\n",
    "    if not ok:\n",
    "        print(\"WARNING: placeholder not found:\", PLACEHOLDER)\n",
    "else:\n",
    "    print(\"NOTE: No summary text found in GPT output (no 'Summary / Interpretation' section).\")\n",
    "\n",
    "# ==============================================\n",
    "# 7) Save\n",
    "# ==============================================\n",
    "out_path = \"/Users/felipesilverio/Documents/GitHub/Azure-OnePager/CompanyProfile2.docx\"\n",
    "doc.save(out_path)\n",
    "print(f\"Updated document written to: {out_path}\")\n",
    "\n",
    "if not_found:\n",
    "    print(\"WARNING — CSV metrics not matched to any row:\")\n",
    "    for m in not_found:\n",
    "        print(\" -\", m)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
